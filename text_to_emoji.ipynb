{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f0a7ce6-99ff-48b2-a74e-2741aade7994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we will implement a simple pretrained model to predict a emoji \n",
    "# given a sentence. This is a starting level implementation.\n",
    "# importing required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import emoji as em\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, SimpleRNN, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6db27ae1-7e0f-4ccc-9e49-1d915527251d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2      55\n",
       "3      43\n",
       "0      28\n",
       "4      22\n",
       "1      18\n",
       "3       9\n",
       "0       4\n",
       "1       1\n",
       "4       1\n",
       "0v2     1\n",
       "2       1\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the data via pandas\n",
    "data = pd.read_csv('./emoji_data.csv', header=None) # as there is no header in data\n",
    "data[1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8471509a-e40c-438b-87a8-e0acb2bc9946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'üëç'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as we can see the data have text and numbers. The numbers represents the \n",
    "# category of the emoji. TO use them, we need emoji librari to create mapping\n",
    "# dictionary\n",
    "# the emoji library has function emojize to print any emoji, such as:\n",
    "em.emojize(':thumbs_up:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7533c07d-4304-42d9-a336-7583fda15246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ':red_heart:',\n",
       " 1: ':baseball:',\n",
       " 2: ':grinning_face_with_big_eyes:',\n",
       " 3: ':disappointed_face:',\n",
       " 4: ':fork_and_knife_with_plate:'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets map the integers values in our data to emojies\n",
    "# lets create a dictionary \n",
    "emoji_dict = {\n",
    "    0:':red_heart:',\n",
    "    1: ':baseball:',\n",
    "    2: ':grinning_face_with_big_eyes:',\n",
    "    3: ':disappointed_face:',\n",
    "    4: ':fork_and_knife_with_plate:'\n",
    "}\n",
    "emoji_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87ae717f-ee1a-43dd-a32d-1e335cf09d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('‚ù§Ô∏è', '‚öæ', 'üòû')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defing a function to where we will pass the label (key) and it will\n",
    "# return us the emoji\n",
    "def label_to_emoji(label):\n",
    "    return em.emojize(emoji_dict[label])\n",
    "\n",
    "label_to_emoji(0), label_to_emoji(1), label_to_emoji(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "449a86e8-a81d-420f-93c3-ddee8438713d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['French macaroon is so tasty', 'work is horrible', 'I am upset',\n",
       "        'throw the ball', 'Good joke',\n",
       "        'what is your favorite baseball game', 'I cooked meat',\n",
       "        'stop messing around', 'I want chinese food',\n",
       "        'Let us go play baseball', 'you are failing this exercise',\n",
       "        'yesterday we lost again', 'Good job', 'ha ha ha it was so funny',\n",
       "        'I will have a cheese cake', 'Why are you feeling bad',\n",
       "        'I want to joke', 'I never said yes for this',\n",
       "        'the party is cancelled', 'where is the ball', 'I am frustrated',\n",
       "        'ha ha ha lol', 'she said yes', 'he got a raise',\n",
       "        'family is all I have', 'he can pitch really well',\n",
       "        'I love to the stars and back', 'do you like pizza ',\n",
       "        'You totally deserve this prize', 'I miss you so much',\n",
       "        'I like your jacket ', 'she got me a present',\n",
       "        'will you be my valentine', 'you failed the midterm',\n",
       "        'Who is down for a restaurant', 'valentine day is near',\n",
       "        'Great so awesome', 'do you have a ball', 'he can not do anything',\n",
       "        'he likes baseball', 'We had such a lovely dinner tonight',\n",
       "        'vegetables are healthy', 'he is a good friend',\n",
       "        'never talk to me again', 'i miss her', 'food is life',\n",
       "        'I am having fun', 'So bad that you cannot come with us',\n",
       "        'do you want to join me for dinner ', 'I like to smile',\n",
       "        'he did an amazing job', 'Stop shouting at me',\n",
       "        'I love taking breaks',\n",
       "        'You are incredibly intelligent and talented',\n",
       "        'I am proud of your achievements', 'So sad you are not coming',\n",
       "        'funny', 'Stop saying bullshit',\n",
       "        'Bravo for the announcement it got a lot of traction',\n",
       "        'This specialization is great',\n",
       "        'I was waiting for her for two hours ',\n",
       "        'she takes forever to get ready ',\n",
       "        'My grandmother is the love of my life', 'I will celebrate soon',\n",
       "        'my code is working but the grader gave me zero',\n",
       "        'She is the cutest person I have ever seen', 'he is laughing',\n",
       "        'I adore my dogs', 'I love you mum', 'great job',\n",
       "        'How dare you ask that', 'this guy was such a joke',\n",
       "        'I love indian food', 'Are you down for baseball this afternoon',\n",
       "        'this is bad', 'Your stupidity has no limit', 'I love my dad',\n",
       "        'Do you want to give me a hug', 'this girl was mean',\n",
       "        'I am excited', 'i miss him', 'What is wrong with you',\n",
       "        'they are so kind and friendly',\n",
       "        'I am so impressed by your dedication to this project',\n",
       "        'we made it', 'I am ordering food', 'Sounds like a fun plan ha ha',\n",
       "        'I am so happy for you', 'Miss you so much', 'I love you',\n",
       "        'this joke is killing me haha',\n",
       "        'You are not qualified for this position', 'miss you my dear',\n",
       "        'I want to eat', 'I am so excited to see you after so long',\n",
       "        'he is the best player', 'What a fun moment',\n",
       "        'my algorithm performs poorly', 'Stop shouting at me',\n",
       "        'her smile is so charming', 'It is the worst day in my life',\n",
       "        'he is handsome', 'no one likes him', 'she is attractive',\n",
       "        'It was funny lol', 'he is so cute', 'you did well on you exam',\n",
       "        'I think I will end up alone', 'Lets have food together',\n",
       "        'too bad that you were not here', 'I want to go play',\n",
       "        'you are a loser', 'I am starving', 'you suck', 'Congratulations',\n",
       "        'you could not solve it', 'I lost my wallet',\n",
       "        'she did not answer my text ', 'That catcher sucks ',\n",
       "        'See you at the restaurant', 'I boiled rice', 'I said yes',\n",
       "        'candy is life ', 'the game just finished',\n",
       "        'The first base man got the ball',\n",
       "        'congratulations on your acceptance',\n",
       "        'The assignment is too long ', 'lol',\n",
       "        'I got humiliated by my sister', 'I want to eat',\n",
       "        'the lectures are great though ', 'you did not do your homework',\n",
       "        'The baby is adorable', 'Bravo', 'I missed you',\n",
       "        'I am looking for a date', 'where is the food', 'you are awful',\n",
       "        'any suggestions for dinner', 'she is happy',\n",
       "        'I am always working', 'This is so funny', 'you got a down grade',\n",
       "        'I want to have sushi for dinner', 'she smiles a lot',\n",
       "        'The chicago cubs won again', 'I got approved', 'cookies are good',\n",
       "        'I hate him', 'I am going to the stadium',\n",
       "        'I am very disappointed', 'I am proud of you forever',\n",
       "        'This girl is messing with me', 'Congrats on the new job',\n",
       "        'enjoy your break', 'go away', 'I worked during my birthday',\n",
       "        'Congratulation fon have a baby', 'I am hungry',\n",
       "        'She is my dearest love', 'she is so cute', 'I love dogs',\n",
       "        'I did not have breakfast ', 'my dog just had a few puppies',\n",
       "        'I like you a lot', 'he had to make a home run',\n",
       "        'I am at the baseball game', 'are you serious ha ha',\n",
       "        'I like to laugh', 'Stop making this joke ha ha ha',\n",
       "        'you two are cute', 'This stupid grader is not working',\n",
       "        'What you did was awesome', 'My life is so boring',\n",
       "        'he did not answer', 'lets exercise', 'you brighten my day',\n",
       "        'I will go dance', 'lets brunch some day', 'dance with me',\n",
       "        'she is a bully', 'she plays baseball',\n",
       "        'I like it when people smile'], dtype=object),\n",
       " array(['4', '3', '3 ', '1 ', '2', '1', '4', '3', '4', '1', '3', '3 ', '2',\n",
       "        '2', '4', '3', '2', '3 ', '3 ', '1', '3 ', '2', '2', '2', '0', '1',\n",
       "        '0', '4 ', '2', '0v2', '2', '0', '0', '3 ', '4', '0', '2', '1',\n",
       "        '3', '1', '0', '4', '0 ', '3', '0 ', '4', '2', '3 ', '4', '2 ',\n",
       "        '2', '3', '0', '2', '2', '3 ', '2', '3', '2', '2', '3 ', '3', '0 ',\n",
       "        '2', '3', '0', '2', '0', '0 ', '2', '3', '2', '4', '1', '3', '3',\n",
       "        '0', '0', '3', '2', '0', '3', '0', '2', '2', '4', '2', '2', '0',\n",
       "        '0', '2', '3', '0', '4', '2', '1', '2', '3', '3', '2', '3', '0',\n",
       "        '3', '0', '2', '0', '2', '3', '4', '3', '1', '3', '4', '3', '2',\n",
       "        '3', '3', '3', '1', '4', '4', '2', '2', '1', '1', '2', '3', '2',\n",
       "        '3', '4', '2', '3', '0', '2', '0', '0', '4', '3', '4', '2', '3',\n",
       "        '2', '3', '4', '2', '1', '2', '4', '3', '1', '3', '2', '3', '2',\n",
       "        '2', '3', '3', '2', '4', '0', '0', '0', '3', '0', '0', '1', '1',\n",
       "        '2', '2', '2', '0', '3', '2', '3', '3', '1', '2', '2', '4', '2',\n",
       "        '3', '1', '2'], dtype=object))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now lets separate the data from labels - here we will use \n",
    "# x -  as data and y - as labels\n",
    "x = data[0].values\n",
    "y = data[1].values\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58dc915a-395a-48e7-a08b-056eedee5202",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# it is time to use word embedding - I will be using pre-trained word embedding\n",
    "# glove, introduced by stanford researchers\n",
    "# please follow this link to download: http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# once it is downloaded: extract it and it will have four files\n",
    "# each file represents the embedding with respect to their dimensions\n",
    "# here, 100d file will be used - lets open it using open()\n",
    "file = open('./glove.6B/glove.6B.100d.txt', encoding='utf8')\n",
    "content = file.readlines()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1436c414-16e1-46aa-9c59-0bcbb4e16559",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459 0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 -0.48336 0.1464 -0.37304 0.34577 0.052041 0.44946 -0.46971 0.02628 -0.54155 -0.15518 -0.14107 -0.039722 0.28277 0.14393 0.23464 -0.31021 0.086173 0.20397 0.52624 0.17164 -0.082378 -0.71787 -0.41531 0.20335 -0.12763 0.41367 0.55187 0.57908 -0.33477 -0.36559 -0.54857 -0.062892 0.26584 0.30205 0.99775 -0.80481 -3.0243 0.01254 -0.36942 2.2167 0.72201 -0.24978 0.92136 0.034514 0.46745 1.1079 -0.19358 -0.074575 0.23353 -0.052062 -0.22044 0.057162 -0.15806 -0.30798 -0.41625 0.37972 0.15006 -0.53212 -0.2055 -1.2526 0.071624 0.70565 0.49744 -0.42063 0.26148 -1.538 -0.30223 -0.073438 -0.28312 0.37104 -0.25217 0.016215 -0.017099 -0.38984 0.87424 -0.72569 -0.51058 -0.52028 -0.1459 0.8278 0.27062\\n',\n",
       " ', -0.10767 0.11053 0.59812 -0.54361 0.67396 0.10663 0.038867 0.35481 0.06351 -0.094189 0.15786 -0.81665 0.14172 0.21939 0.58505 -0.52158 0.22783 -0.16642 -0.68228 0.3587 0.42568 0.19021 0.91963 0.57555 0.46185 0.42363 -0.095399 -0.42749 -0.16567 -0.056842 -0.29595 0.26037 -0.26606 -0.070404 -0.27662 0.15821 0.69825 0.43081 0.27952 -0.45437 -0.33801 -0.58184 0.22364 -0.5778 -0.26862 -0.20425 0.56394 -0.58524 -0.14365 -0.64218 0.0054697 -0.35248 0.16162 1.1796 -0.47674 -2.7553 -0.1321 -0.047729 1.0655 1.1034 -0.2208 0.18669 0.13177 0.15117 0.7131 -0.35215 0.91348 0.61783 0.70992 0.23955 -0.14571 -0.37859 -0.045959 -0.47368 0.2385 0.20536 -0.18996 0.32507 -1.1112 -0.36341 0.98679 -0.084776 -0.54008 0.11726 -1.0194 -0.24424 0.12771 0.013884 0.080374 -0.35414 0.34951 -0.7226 0.37549 0.4441 -0.99059 0.61214 -0.35111 -0.83155 0.45293 0.082577\\n',\n",
       " '. -0.33979 0.20941 0.46348 -0.64792 -0.38377 0.038034 0.17127 0.15978 0.46619 -0.019169 0.41479 -0.34349 0.26872 0.04464 0.42131 -0.41032 0.15459 0.022239 -0.64653 0.25256 0.043136 -0.19445 0.46516 0.45651 0.68588 0.091295 0.21875 -0.70351 0.16785 -0.35079 -0.12634 0.66384 -0.2582 0.036542 -0.13605 0.40253 0.14289 0.38132 -0.12283 -0.45886 -0.25282 -0.30432 -0.11215 -0.26182 -0.22482 -0.44554 0.2991 -0.85612 -0.14503 -0.49086 0.0082973 -0.17491 0.27524 1.4401 -0.21239 -2.8435 -0.27958 -0.45722 1.6386 0.78808 -0.55262 0.65 0.086426 0.39012 1.0632 -0.35379 0.48328 0.346 0.84174 0.098707 -0.24213 -0.27053 0.045287 -0.40147 0.11395 0.0062226 0.036673 0.018518 -1.0213 -0.20806 0.64072 -0.068763 -0.58635 0.33476 -1.1432 -0.1148 -0.25091 -0.45907 -0.096819 -0.17946 -0.063351 -0.67412 -0.068895 0.53604 -0.87773 0.31802 -0.39242 -0.23394 0.47298 -0.028803\\n',\n",
       " 'of -0.1529 -0.24279 0.89837 0.16996 0.53516 0.48784 -0.58826 -0.17982 -1.3581 0.42541 0.15377 0.24215 0.13474 0.41193 0.67043 -0.56418 0.42985 -0.012183 -0.11677 0.31781 0.054177 -0.054273 0.35516 -0.30241 0.31434 -0.33846 0.71715 -0.26855 -0.15837 -0.47467 0.051581 -0.33252 0.15003 -0.1299 -0.54617 -0.37843 0.64261 0.82187 -0.080006 0.078479 -0.96976 -0.57741 0.56491 -0.39873 -0.057099 0.19743 0.065706 -0.48092 -0.20125 -0.40834 0.39456 -0.02642 -0.11838 1.012 -0.53171 -2.7474 -0.042981 -0.74849 1.7574 0.59085 0.04885 0.78267 0.38497 0.42097 0.67882 0.10337 0.6328 -0.026595 0.58647 -0.44332 0.33057 -0.12022 -0.55645 0.073611 0.20915 0.43395 -0.012761 0.089874 -1.7991 0.084808 0.77112 0.63105 -0.90685 0.60326 -1.7515 0.18596 -0.50687 -0.70203 0.66578 -0.81304 0.18712 -0.018488 -0.26757 0.727 -0.59363 -0.34839 -0.56094 -0.591 1.0039 0.20664\\n',\n",
       " 'to -0.1897 0.050024 0.19084 -0.049184 -0.089737 0.21006 -0.54952 0.098377 -0.20135 0.34241 -0.092677 0.161 -0.13268 -0.2816 0.18737 -0.42959 0.96039 0.13972 -1.0781 0.40518 0.50539 -0.55064 0.4844 0.38044 -0.0029055 -0.34942 -0.099696 -0.78368 1.0363 -0.2314 -0.47121 0.57126 -0.21454 0.35958 -0.48319 1.0875 0.28524 0.12447 -0.039248 -0.076732 -0.76343 -0.32409 -0.5749 -1.0893 -0.41811 0.4512 0.12112 -0.51367 -0.13349 -1.1378 -0.28768 0.16774 0.55804 1.5387 0.018859 -2.9721 -0.24216 -0.92495 2.1992 0.28234 -0.3478 0.51621 -0.43387 0.36852 0.74573 0.072102 0.27931 0.92569 -0.050336 -0.85856 -0.1358 -0.92551 -0.33991 -1.0394 -0.067203 -0.21379 -0.4769 0.21377 -0.84008 0.052536 0.59298 0.29604 -0.67644 0.13916 -1.5504 -0.20765 0.7222 0.52056 -0.076221 -0.15194 -0.13134 0.058617 -0.31869 -0.61419 -0.62393 -0.41548 -0.038175 -0.39804 0.47647 -0.15983\\n',\n",
       " 'and -0.071953 0.23127 0.023731 -0.50638 0.33923 0.1959 -0.32943 0.18364 -0.18057 0.28963 0.20448 -0.5496 0.27399 0.58327 0.20468 -0.49228 0.19974 -0.070237 -0.88049 0.29485 0.14071 -0.1009 0.99449 0.36973 0.44554 0.28998 -0.1376 -0.56365 -0.029365 -0.4122 -0.25269 0.63181 -0.44767 0.24363 -0.10813 0.25164 0.46967 0.3755 -0.23613 -0.14129 -0.44537 -0.65737 -0.042421 -0.28636 -0.28811 0.063766 0.20281 -0.53542 0.41307 -0.59722 -0.38614 0.19389 -0.17809 1.6618 -0.011819 -2.3737 0.058427 -0.2698 1.2823 0.81925 -0.22322 0.72932 -0.053211 0.43507 0.85011 -0.42935 0.92664 0.39051 1.0585 -0.24561 -0.18265 -0.5328 0.059518 -0.66019 0.18991 0.28836 -0.2434 0.52784 -0.65762 -0.14081 1.0491 0.5134 -0.23816 0.69895 -1.4813 -0.2487 -0.17936 -0.059137 -0.08056 -0.48782 0.014487 -0.6259 -0.32367 0.41862 -1.0807 0.46742 -0.49931 -0.71895 0.86894 0.19539\\n',\n",
       " 'in 0.085703 -0.22201 0.16569 0.13373 0.38239 0.35401 0.01287 0.22461 -0.43817 0.50164 -0.35874 -0.34983 0.055156 0.69648 -0.17958 0.067926 0.39101 0.16039 -0.26635 -0.21138 0.53698 0.49379 0.9366 0.66902 0.21793 -0.46642 0.22383 -0.36204 -0.17656 0.1748 -0.20367 0.13931 0.019832 -0.10413 -0.20244 0.55003 -0.1546 0.98655 -0.26863 -0.2909 -0.32866 -0.34188 -0.16943 -0.42001 -0.046727 -0.16327 0.70824 -0.74911 -0.091559 -0.96178 -0.19747 0.10282 0.55221 1.3816 -0.65636 -3.2502 -0.31556 -1.2055 1.7709 0.4026 -0.79827 1.1597 -0.33042 0.31382 0.77386 0.22595 0.52471 -0.034053 0.32048 0.079948 0.17752 -0.49426 -0.70045 -0.44569 0.17244 0.20278 0.023292 -0.20677 -1.0158 0.18325 0.56752 0.31821 -0.65011 0.68277 -0.86585 -0.059392 -0.29264 -0.55668 -0.34705 -0.32895 0.40215 -0.12746 -0.20228 0.87368 -0.545 0.79205 -0.20695 -0.074273 0.75808 -0.34243\\n',\n",
       " 'a -0.27086 0.044006 -0.02026 -0.17395 0.6444 0.71213 0.3551 0.47138 -0.29637 0.54427 -0.72294 -0.0047612 0.040611 0.043236 0.29729 0.10725 0.40156 -0.53662 0.033382 0.067396 0.64556 -0.085523 0.14103 0.094539 0.74947 -0.194 -0.68739 -0.41741 -0.22807 0.12 -0.48999 0.80945 0.045138 -0.11898 0.20161 0.39276 -0.20121 0.31354 0.75304 0.25907 -0.11566 -0.029319 0.93499 -0.36067 0.5242 0.23706 0.52715 0.22869 -0.51958 -0.79349 -0.20368 -0.50187 0.18748 0.94282 -0.44834 -3.6792 0.044183 -0.26751 2.1997 0.241 -0.033425 0.69553 -0.64472 -0.0072277 0.89575 0.20015 0.46493 0.61933 -0.1066 0.08691 -0.4623 0.18262 -0.15849 0.020791 0.19373 0.063426 -0.31673 -0.48177 -1.3848 0.13669 0.96859 0.049965 -0.2738 -0.035686 -1.0577 -0.24467 0.90366 -0.12442 0.080776 -0.83401 0.57201 0.088945 -0.42532 -0.018253 -0.079995 -0.28581 -0.01089 -0.4923 0.63687 0.23642\\n',\n",
       " '\" -0.30457 -0.23645 0.17576 -0.72854 -0.28343 -0.2564 0.26587 0.025309 -0.074775 -0.3766 -0.057774 0.12159 0.34384 0.41928 -0.23236 -0.31547 0.60939 0.25117 -0.68667 0.70873 1.2162 -0.1824 -0.48442 -0.33445 0.30343 1.086 0.49992 -0.20198 0.27959 0.68352 -0.33566 -0.12405 0.059656 0.33617 0.37501 0.56552 0.44867 0.11284 -0.16196 -0.94346 -0.67961 0.18581 0.060653 0.43776 0.13834 -0.48207 -0.56141 -0.25422 -0.52445 0.097003 -0.48925 0.19077 0.21481 1.4969 -0.86665 -3.2846 0.56854 0.41971 1.2294 0.78522 -0.29369 0.63803 -1.5926 -0.20437 1.5306 0.13548 0.50722 0.18742 0.48552 -0.28995 0.19573 0.0046515 0.092879 -0.42444 0.64987 0.52839 0.077908 0.8263 -1.2208 -0.34955 0.49855 -0.64155 -0.72308 0.26566 -1.3643 -0.46364 -0.52048 -1.0525 0.22895 -0.3456 -0.658 -0.16735 0.35158 0.74337 0.26074 0.061104 -0.39079 -0.84557 -0.035432 0.17036\\n',\n",
       " \"'s 0.58854 -0.2025 0.73479 -0.68338 -0.19675 -0.1802 -0.39177 0.34172 -0.60561 0.63816 -0.26695 0.36486 -0.40379 -0.1134 -0.58718 0.2838 0.8025 -0.35303 0.30083 0.078935 0.44416 -0.45906 0.79294 0.50365 0.32805 0.28027 -0.4933 -0.38482 -0.039284 -0.2483 -0.1988 1.1469 0.13228 0.91691 -0.36739 0.89425 0.5426 0.61738 -0.62205 -0.31132 -0.50933 0.23335 1.0826 -0.044637 -0.12767 0.27628 -0.032617 -0.27397 0.77764 -0.50861 0.038307 -0.33679 0.42344 1.2271 -0.53826 -3.2411 0.42626 0.025189 1.3948 0.65085 0.03325 0.37141 0.4044 0.35558 0.98265 -0.61724 0.53901 0.76219 0.30689 0.33065 0.30956 -0.15161 -0.11313 -0.81281 0.6145 -0.44341 -0.19163 -0.089551 -1.5927 0.37405 0.85857 0.54613 -0.31928 0.52598 -1.4802 -0.97931 -0.2939 -0.14724 0.25803 -0.1817 1.0149 0.77649 0.12598 0.54779 -1.0316 0.064599 -0.37523 -0.94475 0.61802 0.39591\\n\",\n",
       " 'for -0.14401 0.32554 0.14257 -0.099227 0.72536 0.19321 -0.24188 0.20223 -0.89599 0.15215 0.035963 -0.59513 -0.051635 -0.014428 0.35475 -0.31859 0.76984 -0.087369 -0.24762 0.65059 -0.15138 -0.42703 0.18813 0.091562 0.15192 0.11303 -0.15222 -0.62786 -0.23923 0.096009 -0.46147 0.41526 -0.30475 0.1371 0.16758 0.53301 -0.043658 0.85924 -0.41192 -0.21394 -0.51228 -0.31945 0.12662 -0.3151 0.0031429 0.27129 0.17328 -1.3159 -0.42414 -0.69126 0.019017 -0.13375 -0.096057 1.7069 -0.65291 -2.6111 0.26518 -0.61178 2.095 0.38148 -0.55823 0.2036 -0.33704 0.37354 0.6951 -0.001637 0.81885 0.51793 0.27746 -0.37177 -0.43345 -0.42732 -0.54912 -0.30715 0.18101 0.2709 -0.29266 0.30834 -1.4624 -0.18999 0.92277 -0.099217 -0.25165 0.49197 -1.525 0.15326 0.2827 0.12102 -0.36766 -0.61275 -0.18884 0.10907 0.12315 0.090066 -0.65447 -0.17252 2.6336e-05 0.25398 1.1078 -0.073074\\n',\n",
       " '- -1.2557 0.61036 0.56793 -0.96596 -0.45249 -0.071696 0.57122 -0.31292 -0.43814 0.90622 0.06961 -0.053104 0.25029 0.27841 0.77724 0.26329 0.56874 -1.1171 -0.078268 -0.51317 0.8071 0.99214 0.22753 1.0847 0.88292 0.17221 -0.68686 -0.86467 -0.80003 -0.34738 -0.044074 -0.30444 0.23406 0.28592 0.060548 -0.65477 -0.039738 0.74878 -0.46471 0.063023 -0.16519 -1.2217 -0.089479 -0.8125 0.27615 -0.13841 -0.76667 -0.96974 0.83123 -0.77639 -1.3327 -0.28732 -0.053684 1.1735 -1.1795 -2.7519 0.45359 1.1984 2.8203 0.060114 0.32296 0.19097 0.3459 -0.41503 0.1515 0.38148 1.619 0.9929 -0.82549 -0.098692 0.74449 -0.38602 -1.0004 -1.305 -0.31269 -0.57625 0.14095 -0.80269 -1.4714 -0.48014 1.1993 -0.48561 0.40496 -0.032867 -2.051 0.18284 -0.2723 0.043287 0.066801 -0.62832 -0.05854 0.28253 -0.083276 -0.022234 -0.55914 0.24586 0.36052 -1.5877 0.76984 -0.64998\\n',\n",
       " 'that -0.093337 0.19043 0.68457 -0.41548 -0.22777 -0.11803 -0.095434 0.19613 0.17785 -0.020244 -0.055409 0.33867 0.79396 -0.047126 0.44281 -0.061266 0.20796 0.034094 -0.64751 0.35874 0.13936 -0.6831 0.25596 -0.12911 0.2608 -0.11674 0.024925 -0.60259 -0.41474 -0.51104 0.14936 0.79977 -0.12716 0.40474 -0.21435 0.47031 0.49 0.48886 -0.17772 -0.18861 -0.78391 -0.14158 0.22169 -0.22078 -0.30509 -0.10837 0.57168 -0.7832 -0.16328 -0.76131 0.080873 0.00067217 0.44713 1.3434 -0.20014 -2.868 -0.002647 -0.39858 1.8379 1.2211 -0.16066 0.65853 0.26946 0.27212 0.94735 0.24372 0.8194 0.6774 0.063485 -0.55934 0.45541 -0.64684 -0.034702 -0.45566 0.21847 -0.051689 0.32299 -0.022961 -1.7955 0.31217 0.76227 -0.23191 -1.0133 -0.0064374 -1.8135 -0.75221 0.28362 -0.30815 -0.43853 -0.62654 0.13213 -0.54725 -0.47478 -0.0079727 -0.15112 -0.29326 -0.35118 -0.68175 0.28804 0.54893\\n',\n",
       " 'on -0.21863 -0.42664 0.5196 0.0043103 0.58045 -0.10873 -0.37726 0.4566 -0.60627 -0.075773 0.11306 0.17703 0.1605 0.074514 0.63649 -0.078852 0.75268 -0.24962 -0.51628 -0.33348 0.66754 -0.34183 0.61316 0.31668 0.64846 -0.079312 -0.065219 -0.17718 -0.32439 0.51868 -0.23424 0.34381 0.046851 0.74025 -0.47005 0.53685 -0.35549 0.40737 -0.093421 -0.13439 -0.41969 -0.30041 0.28646 0.37419 -0.46054 -0.307 -0.3858 -0.69317 -0.00092461 -0.61984 0.11978 0.1495 0.17833 1.5313 -0.92445 -3.0428 0.030761 -0.64359 2.3824 0.56219 -0.56021 1.0264 -0.45143 0.14117 0.65944 0.37572 0.098334 0.38304 -0.076882 -0.21781 -0.29892 -0.49458 0.095239 -0.63059 -0.061311 0.17767 -0.14051 0.47182 -0.95891 0.045334 0.808 -0.026867 -0.27483 0.35541 -0.82896 -0.78838 -0.079732 0.22941 -0.45013 -0.3004 -0.52716 0.11358 -0.49906 0.827 -0.56991 0.25143 -0.40266 -0.29146 1.3816 0.18084\\n',\n",
       " 'is -0.54264 0.41476 1.0322 -0.40244 0.46691 0.21816 -0.074864 0.47332 0.080996 -0.22079 -0.12808 -0.1144 0.50891 0.11568 0.028211 -0.3628 0.43823 0.047511 0.20282 0.49857 -0.10068 0.13269 0.16972 0.11653 0.31355 0.25713 0.092783 -0.56826 -0.52975 -0.051456 -0.67326 0.92533 0.2693 0.22734 0.66365 0.26221 0.19719 0.2609 0.18774 -0.3454 -0.42635 0.13975 0.56338 -0.56907 0.12398 -0.12894 0.72484 -0.26105 -0.26314 -0.43605 0.078908 -0.84146 0.51595 1.3997 -0.7646 -3.1453 -0.29202 -0.31247 1.5129 0.52435 0.21456 0.42452 -0.088411 -0.17805 1.1876 0.10579 0.76571 0.21914 0.35824 -0.11636 0.093261 -0.62483 -0.21898 0.21796 0.74056 -0.43735 0.14343 0.14719 -1.1605 -0.050508 0.12677 -0.014395 -0.98676 -0.091297 -1.2054 -0.11974 0.047847 -0.54001 0.52457 -0.70963 -0.32528 -0.1346 -0.41314 0.33435 -0.0072412 0.32253 -0.044219 -1.2969 0.76217 0.46349\\n',\n",
       " 'was 0.13717 -0.54287 0.19419 -0.29953 0.17545 0.084672 0.67752 0.098295 -0.035611 0.21334 0.51663 0.20687 0.44082 -0.33655 0.56025 -0.6879 0.51957 -0.21258 -0.52708 -0.12249 0.33099 0.026448 0.59007 0.0065469 0.45405 -0.33884 -0.28261 -0.24633 0.10847 0.3164 -0.15368 0.73503 0.11858 0.70842 0.075081 0.29738 -0.11395 0.40807 -0.042531 -0.21301 -0.79849 -0.12703 0.752 -0.41746 0.46615 -0.039097 0.65961 -0.32336 0.442 -0.94137 -0.23125 -0.30604 0.79912 1.4581 -0.88199 -3.0041 -0.75243 -0.20503 1.1998 0.94881 0.30649 0.48411 -0.7572 0.65856 0.70107 -0.93141 0.52928 0.23323 0.18857 0.38691 0.011489 -0.31937 0.011858 0.22944 0.17764 0.16868 0.14003 0.58647 -1.5447 -0.064425 -0.00064711 0.13606 -0.32695 0.10043 -1.546 -0.5476 0.21027 -0.67195 -0.1597 -0.68271 -0.22043 -0.87088 -0.16248 0.83086 -0.23045 0.19864 -0.051892 -0.52057 0.25434 -0.23759\\n',\n",
       " 'said -0.13128 -0.452 0.043399 -0.99798 -0.21053 -0.95868 -0.24609 0.48413 0.18178 0.475 -0.22305 0.30064 0.43496 -0.3605 0.20245 -0.52594 -0.34708 0.0075873 -1.0497 0.18673 0.57369 0.43814 0.098659 0.3877 -0.2258 0.41911 0.043602 -0.7352 -0.53583 0.19276 -0.21961 0.42515 -0.19082 0.47187 0.18826 0.13357 0.41839 1.3138 0.35678 -0.32172 -1.2257 -0.26635 0.36716 -0.27586 -0.53246 0.16786 -0.11253 -0.99959 -0.60706 -0.89271 0.65156 -0.88784 0.049233 0.67111 -0.27553 -2.4005 -0.36989 0.29136 1.3498 1.7353 0.27 0.021299 0.14422 0.023784 0.33643 -0.35476 1.0921 1.4845 0.4943 0.15688 0.34679 -0.57221 0.12093 -1.2616 1.0541 0.064335 -0.002732 0.19038 -1.7643 0.055068 1.4737 -0.41782 -0.57342 -0.12129 -1.3169 -0.73883 0.17682 -0.019991 -0.49176 -0.55247 1.0623 -0.62879 0.29098 0.13238 -0.70414 0.67128 -0.085462 -0.30526 -0.045495 0.56509\\n',\n",
       " 'with -0.43608 0.39104 0.51657 -0.13861 0.2029 0.50723 -0.012544 0.22948 -0.6316 0.21199 -0.018043 -0.39364 0.74164 0.30221 0.51792 -0.25191 0.25373 -0.65184 -0.42963 0.0093622 0.023334 -0.39245 0.34948 0.21217 0.7346 -0.21962 -0.028611 -0.34641 -0.20934 -0.27091 -0.17637 0.82396 -0.082339 -0.034869 0.079722 0.34841 0.60887 0.22811 -0.29633 0.18633 0.234 -0.70966 0.16312 -0.20857 0.092369 -0.075435 -0.13905 -0.35121 -0.19972 -0.41687 -0.31485 0.16123 0.038882 1.6654 -0.12401 -3.3419 0.10929 -0.026199 1.244 0.84374 -0.15679 0.79041 -0.042433 0.18884 0.064345 -0.11683 1.0467 0.71813 0.57834 0.27014 -0.50908 -0.083995 -0.1437 -0.76408 0.27418 0.56814 -0.39375 -0.32558 -0.92854 -0.13098 1.3277 0.11851 -0.15551 0.5972 -1.084 -0.058137 0.23886 0.14558 -0.59303 -0.47511 -0.22064 -0.37591 -0.79649 0.013465 -0.44595 -0.34623 -0.75398 -0.3517 0.99456 0.088196\\n',\n",
       " 'he 0.1225 -0.058833 0.23658 -0.28877 -0.028181 0.31524 0.070229 0.16447 -0.027623 0.25214 0.21174 -0.059674 0.36133 0.13607 0.18755 -0.1487 0.31315 0.13368 -0.59703 -0.030161 0.080656 0.26162 -0.055924 -0.35351 0.34722 -0.0055801 -0.57935 -0.88007 0.42931 -0.15695 -0.51256 1.2684 -0.25228 0.35265 -0.46419 0.55648 -0.57556 0.32574 -0.21893 -0.13178 -1.1027 -0.039591 0.89643 -0.9845 -0.47393 -0.12855 0.63506 -0.94888 0.40088 -0.77542 -0.35153 -0.27788 0.68747 1.458 -0.38474 -2.8937 -0.29523 -0.38836 0.94881 1.3891 0.054591 0.70486 -0.65699 0.075648 0.7655 -0.63365 0.86556 0.42441 0.14796 0.4156 0.29354 -0.51295 0.19635 -0.45568 0.0080246 0.14528 -0.15395 0.11406 -1.2167 -0.1111 0.8264 0.21738 -0.63776 -0.074874 -1.713 -0.8827 -0.0073058 -0.37623 -0.50209 -0.58844 -0.24943 -1.0425 0.27678 0.64142 -0.64605 0.43559 -0.37276 -0.0032068 0.18744 0.30702\\n',\n",
       " 'as -0.32721 0.096446 0.34244 -0.44327 0.30535 -0.042016 -0.071235 -0.31036 -0.22557 -0.181 -0.29088 -0.61542 0.29751 0.030491 0.41504 -0.51489 0.68628 -0.020302 -0.18486 0.31605 0.59472 -0.2147 0.29256 0.43262 0.35466 -0.29659 -0.27086 -0.48953 -0.047391 0.24521 -0.15783 0.59742 -0.41664 0.057632 0.1233 0.62326 -0.08844 0.3077 -0.15742 -0.28381 -0.58058 -0.022824 0.26689 -0.22565 0.47548 0.11134 0.37263 -0.14554 -0.16775 -0.79377 -0.30593 -0.10671 0.44199 1.5698 -0.73062 -2.7314 -0.19366 -0.32983 1.2881 0.62126 -0.255 0.8416 -0.23658 0.42594 0.86589 -0.35904 0.78162 0.20396 0.82898 0.0016123 -0.24008 -0.72735 -0.053671 -0.22264 0.31034 -0.21243 -0.14335 0.317 -0.80478 -0.49311 0.88023 -0.24147 -0.3922 0.15997 -1.5854 -0.25824 0.052834 -0.11983 -0.018874 -0.77356 0.049285 -0.25332 -0.3073 0.51295 -0.56802 -0.21239 -0.39741 -0.38165 0.43994 0.24683\\n',\n",
       " 'it -0.30664 0.16821 0.98511 -0.33606 -0.2416 0.16186 -0.053496 0.4301 0.57342 -0.071569 0.36101 0.26729 0.27789 -0.072268 0.13838 -0.26714 0.12999 0.22949 -0.18311 0.50163 0.44921 -0.020821 0.42642 -0.068762 0.40337 0.095198 -0.31944 -0.54651 -0.13345 -0.56511 -0.20975 1.1592 -0.194 0.19828 -0.11924 0.41781 0.0068383 -0.20537 -0.53375 -0.52225 -0.38227 -0.0065833 0.14265 -0.42502 -0.3115 0.0027352 0.75093 -0.48218 -0.18595 -0.77104 -0.046406 -0.06914 0.41688 1.3235 -0.81742 -3.3998 -0.11307 -0.34123 2.0775 0.61369 0.14792 0.93753 -0.10138 0.28426 0.97899 -0.32335 0.63697 0.58308 0.2282 -0.31696 0.21061 -0.6506 0.21653 -0.24347 0.55519 -0.34351 -0.095093 -0.14715 -1.2876 0.3931 0.30163 -0.21767 -1.1146 0.51349 -1.341 -0.30381 0.32499 -0.45236 -0.17743 -0.048504 -0.12178 -0.42108 -0.40327 0.038452 -0.36084 0.037738 -0.21885 -0.38775 0.36916 0.54521\\n',\n",
       " 'by -0.20875 -0.1174 0.26478 -0.28339 0.19584 0.7446 -0.03887 0.028499 -0.44252 -0.30426 0.27133 -0.51907 0.52183 -0.76648 0.28043 -0.48344 -0.15626 -0.49705 -0.51024 -0.03652 0.20579 -0.6136 0.46388 0.73497 0.66813 -0.4443 -0.17603 -0.5478 -0.013521 0.16333 0.28148 0.054223 -0.19906 -0.1907 -0.43179 0.14781 0.27555 0.18571 -0.40776 -0.15415 -0.5885 -0.0085281 -0.14178 0.7061 0.54031 -0.43305 0.17497 -0.46208 -0.31372 -0.34039 -0.25128 0.68228 0.33576 1.5862 -0.39427 -2.9938 -0.29773 0.04213 1.9075 -0.072628 -0.092191 0.66133 0.13868 0.78774 0.69307 -0.22185 0.71705 1.1453 1.2153 0.14196 -0.79914 0.16965 -0.34532 -0.51742 -0.15648 0.18757 0.1694 -0.0082713 -1.4511 0.061983 1.1019 0.084411 -0.34148 0.49994 -1.1106 -0.13759 0.15377 -0.061006 -0.53826 -0.78941 -0.12566 -0.57381 -0.73484 0.54774 -0.28455 -0.24348 -0.27511 -0.33267 0.27878 -0.8705\\n',\n",
       " 'at 0.1766 0.093851 0.24351 0.44313 -0.39037 0.12524 -0.19918 0.59855 -0.82035 0.28006 0.54231 0.023079 0.12837 -0.044489 0.3837 -0.75659 0.40254 -0.4462 -0.81599 -0.0091513 0.65219 -0.043656 0.54919 -0.16696 0.73028 -0.20703 -0.069863 -0.31259 0.27226 0.084905 -0.60498 0.42826 0.60134 0.50953 -0.39073 0.44608 -0.36331 0.50858 -0.20308 -0.43503 -0.086827 -0.86581 -1.0151 -0.35725 -0.12993 0.3324 0.3026 0.067277 -0.52948 -0.81223 0.39562 -0.79537 0.24331 1.2506 -1.0169 -3.3391 -0.79691 -0.33877 1.366 0.87513 -0.63701 0.68381 -0.057432 0.12541 -0.8258 -0.56117 0.30807 0.1545 0.61473 0.67403 -0.60833 -0.25911 -0.35619 -0.71189 -0.31207 0.035238 0.22488 -0.33492 -1.1586 -0.17373 0.95937 0.24479 -0.46205 -0.075941 -1.0844 0.093676 0.48546 0.13008 0.23455 -0.27964 -0.24481 -0.016213 0.46302 1.0291 -0.81817 0.17522 0.06797 0.056305 1.2312 0.40695\\n',\n",
       " '( 0.19247 0.36617 0.52301 -0.79857 -0.2592 0.18267 0.19564 0.83148 -0.67636 -0.84648 1.4429 -0.84978 -0.023986 1.328 0.74061 0.039546 0.61659 -0.075604 -0.59537 0.69163 0.71303 0.016798 0.57518 0.94396 0.38447 -0.095771 0.40682 -0.1736 0.29918 0.023185 -0.76169 1.1022 0.25427 -0.59429 0.19951 0.0047585 0.55357 1.3464 0.46901 0.35119 -0.15608 -0.74119 -0.454 -0.4307 0.49688 -0.97074 -0.45785 -0.19753 -0.25268 -0.22272 0.69589 0.37725 0.70939 0.71797 -1.0762 -2.2584 0.21776 -0.086578 1.0148 0.4768 0.56119 0.80465 -1.0638 0.54387 0.84943 0.051364 -0.21503 -0.25736 0.67899 0.31385 -0.02978 0.38168 -0.68692 -0.18547 0.70286 -0.77652 -0.19728 0.45196 -1.2119 -0.28732 -0.027681 -0.66313 -0.39999 -0.29214 -0.72999 1.3007 -0.090283 0.093152 1.0336 -0.29074 -0.24114 0.21022 0.031178 0.20095 -0.75632 0.343 -0.024949 -1.2276 1.1152 -1.0234\\n',\n",
       " ') -0.13797 0.27084 0.84036 -0.45668 -0.49429 0.35777 0.077772 0.42481 0.0076481 -0.50942 1.4008 -0.79993 0.053011 1.2054 0.3783 0.0842 0.91317 -0.35173 -0.30452 0.65606 0.50428 0.074796 0.31149 0.81957 0.40216 0.10982 0.39245 -0.47063 0.28423 0.13887 -1.0394 1.0172 -0.20146 -0.2723 0.071315 -0.48863 0.55583 0.85816 -0.04158 0.31446 -0.12432 -0.80119 -0.015955 -0.58919 0.32295 -1.0475 -0.53598 -0.77031 -0.39489 -0.27798 0.10422 -0.28293 0.72327 1.2162 -0.67242 -2.6729 -0.30428 0.039434 0.99252 0.29029 0.14169 0.29153 -0.47012 0.54204 0.71572 -0.29337 -0.034427 -0.059365 1.1216 0.41185 -0.094306 -0.21824 -0.027208 -0.53825 0.037405 -0.8403 0.035407 0.51239 -1.0315 -0.24264 -0.42759 -0.77984 -0.58269 0.026131 -0.94419 0.91568 0.16742 -0.45787 0.76123 -0.043248 -0.28311 0.063622 -0.24463 0.1474 -0.82823 0.57946 -0.1286 -0.96056 0.3906 -0.92805\\n',\n",
       " 'from 0.30731 0.24737 0.68231 -0.52367 0.44053 0.42044 0.0002514 0.15265 -0.61363 0.22631 0.083071 0.070425 0.017683 0.56807 1.0067 -0.46206 0.44524 -0.50984 -0.42985 0.19935 0.22729 0.51662 0.56282 0.41282 0.17742 -0.15694 -0.11505 -0.3805 0.4744 -0.16686 0.23153 0.063698 -0.10716 -0.26848 -0.42665 0.52237 0.095376 0.6402 -0.52221 -0.13856 -0.98307 -0.3532 -0.52161 0.11277 0.31634 0.13297 -0.049571 -0.13785 0.11317 -0.50644 0.38373 0.36698 0.39106 0.98143 -0.5441 -2.464 -0.68383 -0.96243 2.2017 0.56643 -0.04941 1.3093 -0.40073 0.8353 0.1744 0.044926 0.54118 -0.11038 0.382 0.15369 -0.37072 -0.13141 -0.52504 -0.56775 -0.16822 -0.091726 0.081418 0.045884 -1.4401 -0.16349 0.49361 0.2141 -0.7011 0.23067 -1.1803 0.065701 -0.046429 0.080979 -0.16424 -0.72896 -0.21221 0.034235 -0.40642 0.28826 -0.81331 -0.067997 -0.25439 0.13735 1.0103 -0.77614\\n',\n",
       " 'his 0.12883 -0.82209 0.27438 -0.069014 0.17989 0.72605 -0.15112 0.0085541 -0.95122 0.77243 -0.28375 0.28329 0.14825 -0.01223 -0.019267 -0.03446 0.31506 -0.16639 -0.013435 -0.0020459 0.064905 -0.20989 0.12524 0.3523 0.6404 0.05957 -0.80302 -0.81648 0.66134 0.05997 -0.061521 0.84922 -0.028733 0.2767 -1.0068 0.71758 -0.37257 0.43064 -0.49244 0.38683 -0.36828 0.027982 1.5346 -0.60533 -0.34449 -0.17069 0.29288 -0.53581 0.56035 -0.63013 -0.12308 0.093633 0.59336 1.5214 -0.092629 -3.1408 0.13931 -0.5382 1.1736 0.62318 0.43621 1.2856 0.12121 0.46206 0.56142 -0.41439 0.9436 0.38954 -0.053156 0.18622 -0.18785 0.37603 0.13878 -1.2881 0.18534 0.35157 -0.80888 -0.067662 -1.1934 0.20095 0.96297 0.92074 -0.030933 -0.11743 -1.521 -0.77539 -0.091178 -0.12774 -0.63958 -0.68099 -0.16037 -0.21732 0.57088 0.86688 -0.67851 -0.60641 -0.68927 -0.33961 0.42743 0.16575\\n',\n",
       " \"'' 0.16478 0.17071 0.62111 -1.2101 -0.84063 0.21893 0.48123 -0.15044 0.36701 -0.20857 -0.23385 0.019356 -0.045098 0.18001 0.11995 -0.25622 -0.026299 0.28473 -0.91322 0.59811 0.30248 0.27973 0.11444 -0.073628 0.88137 1.0633 -0.22116 -0.7982 -0.137 -0.2935 0.30011 -0.027594 0.13646 0.0495 0.066336 0.62306 0.4118 0.090881 0.20817 -1.1378 -0.15726 -0.27827 -0.16412 0.50816 -0.75323 -0.33559 -0.14433 -0.85375 -0.76168 -0.65671 0.063944 0.046424 0.15268 1.3671 0.057888 -2.7643 0.45043 0.95083 1.9512 0.66661 -0.32556 1.1692 -0.058985 -0.4976 0.82273 -0.43086 1.5947 0.97728 0.57046 -0.012486 0.63458 -0.67646 -0.22447 -0.41826 0.48309 -0.003709 0.040551 0.13703 -0.93292 -0.24249 0.69614 -0.1772 -0.49188 -0.1192 -1.3653 0.082923 -0.26056 -0.43172 -0.28799 -0.56304 0.065289 -0.56735 0.22621 0.47139 -0.48157 0.56438 0.34019 -0.44047 -0.10587 0.79509\\n\",\n",
       " '`` 0.092672 0.20241 0.69394 -0.50775 -0.097297 0.045522 -0.14156 0.30736 -0.35448 -0.20612 -0.21092 -0.0026685 -0.11537 0.052913 0.02908 0.0067036 0.47268 0.44669 -0.35419 0.70959 0.6984 0.42713 -0.40276 -0.37443 0.7434 0.67827 -0.53675 -0.96641 -0.42534 -0.26468 0.25737 0.57259 0.072823 0.45968 0.080224 0.34628 0.69218 0.28035 0.484 -0.36537 -0.3727 0.4901 -0.2787 -0.22158 -0.45082 -0.35663 -0.18381 -0.50743 -0.62373 -1.084 -0.14465 -0.049544 0.1102 0.81705 -0.85176 -2.8731 0.72268 0.31316 1.625 1.1221 -0.13595 1.0558 -0.49829 -0.62308 1.1854 -0.52551 1.1743 0.62574 -0.14725 0.25494 0.27639 -0.57139 -0.29194 -0.5177 0.18006 0.024181 0.16562 0.25703 -1.151 -0.18682 0.99148 -0.34432 -0.53266 0.47968 -1.8672 -0.52944 -0.049901 -0.48973 -0.60601 -0.63287 0.14499 -0.10127 0.25294 0.37802 -0.39809 -0.4138 -0.05448 -0.51348 -0.2775 1.1636\\n',\n",
       " 'an -0.4214 -0.18797 0.46241 -0.17605 0.36212 0.36701 0.27924 0.14634 -0.054227 0.45834 0.065416 -0.33725 0.067505 -0.36316 0.50302 -0.010361 0.72826 -0.17564 -0.33996 0.072864 0.64481 -0.23908 0.38383 0.13858 1.0994 -0.24883 -0.15078 -0.48738 -0.23042 0.064788 -0.70183 0.82654 0.06128 0.18531 -0.30162 -0.022151 0.34302 0.80331 0.17135 0.15462 -0.50759 0.39572 0.054291 -0.53081 0.48252 0.086205 0.59585 -0.22377 -0.3955 -0.73036 -0.10279 -0.39166 1.229 1.2129 -1.0365 -3.4971 0.10923 -1.0084 1.9998 0.7964 0.3881 0.43746 0.085194 0.38549 0.61993 -1.032 0.70119 -0.2246 0.079435 0.09126 -0.21196 -0.55429 -0.053352 -0.80201 0.46798 -0.05005 -0.57422 -0.084822 -1.7227 -0.94286 0.98667 0.31211 -0.37735 0.068674 -0.77838 -0.28486 0.81047 0.46596 -0.11865 -0.93411 0.33722 0.037906 -0.18273 -0.019941 0.20494 -0.47718 -0.49253 -0.56518 0.72558 -0.15913\\n',\n",
       " 'be -0.46953 0.38432 0.54833 -0.63401 0.010133 0.11364 0.10612 0.58529 0.032302 -0.12274 0.030265 0.52662 1.0398 -0.082143 0.19118 -0.83784 0.50763 0.44488 -0.72604 0.036893 0.24211 -0.28878 0.33657 0.13656 0.14579 -0.13221 0.098428 -0.45276 -0.13029 0.015762 -0.010161 0.4967 -0.28461 0.29655 0.92979 0.42447 -0.082773 0.30438 -0.39219 -0.30585 -0.43201 -0.27333 0.24388 -0.58081 0.22679 0.027226 0.53473 -0.37527 -0.16119 -1.1235 0.12768 -0.69898 0.41341 1.2291 -0.41248 -2.5173 -0.15354 -0.043107 1.9111 0.80754 -0.14759 0.9609 -0.84267 0.084422 1.2616 -0.10938 0.54846 0.75255 -0.071289 -0.73987 0.094808 -0.97589 0.0078721 -0.23928 0.2882 -0.41516 0.034366 0.1197 -1.2142 -0.11306 0.52847 -0.42273 -0.93378 -0.046645 -2.122 -0.341 0.64229 -0.10097 -0.22875 -1.0776 -0.68044 -0.26372 -0.18331 -0.051632 -0.30836 0.066537 0.20422 -0.68914 0.4511 0.25125\\n',\n",
       " 'has 0.093736 0.56152 0.48364 -0.45987 0.56067 -0.1694 0.018687 0.45529 0.065615 0.25181 -0.14251 0.10532 0.77865 0.1428 -0.08114 -0.069555 0.32433 0.019611 -0.15608 0.22235 0.35559 0.14713 0.19156 0.2803 0.27691 -0.2067 -0.11378 -0.48318 -0.64248 -0.35523 0.21939 1.2533 -0.21164 0.91811 0.31986 0.48367 0.15322 0.56109 -0.60692 -0.028075 -0.92199 -0.25583 0.66362 -0.49082 0.34757 -0.048103 0.57283 -0.62332 0.87508 -0.50079 -0.12316 -0.69096 0.10129 1.516 -0.174 -2.8902 -0.24541 -0.17934 1.1001 1.4198 0.49132 0.30282 0.077149 -0.097834 0.90586 -0.1615 0.55681 0.32817 0.49335 0.044815 0.57458 -0.32663 -0.29745 0.001807 0.24382 -0.51915 -0.14392 0.27921 -1.5964 0.37152 0.81129 -0.13488 -0.36534 -0.022346 -1.5091 -0.38727 0.30063 -0.37562 -0.18582 -0.39748 -0.10719 -0.12265 -0.66462 0.12112 -0.37281 0.60048 -0.42683 -0.81305 0.62397 0.73176\\n',\n",
       " 'are -0.51533 0.83186 0.22457 -0.73865 0.18718 0.26021 -0.42564 0.67121 -0.31084 -0.61275 0.089526 -0.24011 1.1878 0.67609 -0.022885 -0.92533 0.071174 0.38837 -0.42924 0.37144 0.32671 0.43141 0.87495 0.34009 -0.23189 -0.41144 0.49061 -0.32906 -0.49109 -0.18988 0.33408 -0.21245 -0.38386 -0.080547 1.1161 0.23617 0.31333 0.49286 0.1 -0.15131 -0.14176 -0.2802 -0.2388 -0.35486 0.18282 -0.19134 0.60544 0.074573 -0.20731 -0.60965 0.19908 -0.57024 -0.17427 1.4419 -0.25019 -1.8648 0.41671 -0.24607 1.501 0.87415 -0.67135 1.2762 -0.2721 0.17583 1.2242 0.28242 0.62375 0.63951 0.36914 -0.84677 -0.3227 -0.67152 -0.19635 -0.40789 -0.20966 -0.19623 0.041885 0.53967 -1.1105 -0.39515 0.6659 -0.233 -1.082 0.046465 -2.0993 -0.28493 0.080025 -0.12963 -0.30011 -0.46764 -0.81831 -0.048509 -0.32233 -0.32013 -1.1207 -0.056788 -0.73004 -1.2024 1.1304 0.3479\\n',\n",
       " 'have 0.15711 0.65606 0.0021149 -0.65144 -0.28427 -0.20369 -0.077596 0.40798 -0.03447 -0.1639 -0.21597 0.34178 1.196 0.33639 -0.21076 -0.56015 0.1507 0.34912 -0.97128 0.18152 0.74408 0.20029 0.66986 0.16085 -0.0013507 -0.55392 0.19411 -0.48043 -0.29777 -0.50765 0.80164 0.50424 -0.40524 0.53991 0.65686 0.2114 0.18575 0.80697 -0.20066 0.095714 -0.58899 -0.35907 0.27162 -0.51794 0.2347 -0.045999 0.56501 -0.40747 0.63377 -0.92266 -0.023418 -0.2504 -0.19576 1.3863 0.087314 -2.2309 0.15084 -0.18661 1.292 1.3259 -0.3018 1.2554 -0.41594 0.045082 1.2569 0.19148 0.53144 0.54904 0.16331 -0.66509 0.11798 -0.52498 -0.084623 -0.55866 -0.44294 -0.19599 -0.17698 0.3181 -1.4736 0.26293 0.96367 -0.25463 -0.70786 -0.11713 -2.3508 -0.47386 0.56258 -0.079965 -0.69856 -0.48524 -0.33081 -0.19205 -0.49695 -0.32643 -0.97207 0.21092 -0.58082 -0.60615 0.71005 0.41469\\n',\n",
       " 'but -0.057078 0.39874 0.68861 -0.68151 -0.45583 0.2008 0.17974 0.053648 0.43762 -0.026725 0.13383 -0.0078137 0.42207 -0.31801 0.18065 -0.35387 -0.30929 0.04066 -0.48854 0.3791 0.47955 -0.041942 0.40894 0.12419 0.40096 0.19545 -0.37819 -0.77684 -0.20677 -0.4313 -0.10095 0.39866 -0.29612 -0.083111 -0.019026 0.53927 0.0011912 0.30235 -0.36048 -0.48434 -0.47751 -0.33922 0.34788 -0.17484 -0.22613 -0.3291 0.81259 -0.58452 0.14509 -0.71497 0.17107 -0.24833 0.22104 1.5517 0.040869 -2.9103 -0.20812 -0.17625 1.6597 0.86277 -0.32527 0.65641 -0.13142 0.32312 0.90836 -0.29105 0.84975 0.53217 0.15041 -0.27983 -0.029015 -0.63378 0.12237 -0.79144 0.16108 0.017446 -0.35095 -0.16949 -1.0001 -0.036832 0.8114 -0.2271 -0.62133 0.16484 -1.6804 -0.39861 0.063602 0.10644 -0.57955 -0.45573 -0.037633 -0.63445 -0.30094 0.39828 -0.82883 0.33827 -0.23613 -0.19357 -0.030606 0.2397\\n',\n",
       " 'were 0.26874 0.17994 -0.29083 -0.72304 -0.05883 0.37211 0.39979 0.47827 -0.41014 -0.089043 0.68457 0.29088 0.9661 0.43289 0.44254 -1.1529 0.15147 -0.02307 -1.2467 -0.037292 0.94212 0.37771 1.2369 0.12327 -0.33831 -0.98651 0.44322 0.083459 -0.11953 -0.057447 0.6761 -0.59646 -0.3251 0.53957 0.66822 0.082015 0.42181 0.62666 0.038678 0.089652 -0.53395 -0.40426 -0.060807 0.14335 0.53841 -0.12983 0.43699 -0.077531 0.20441 -0.9894 -0.080389 -0.13893 0.046432 1.6775 -0.34565 -1.7503 -0.25442 -0.28207 1.2024 1.0927 -0.55076 1.3852 -0.74759 0.96273 0.69044 -0.41462 0.55676 0.39588 0.053647 -0.35503 -0.3909 -0.48323 -0.048448 -0.37728 -0.51204 0.50097 0.16188 0.91052 -1.6308 -0.31484 0.51824 -0.078027 -0.33929 0.42289 -2.3287 -0.56737 0.17769 -0.34047 -0.75328 -0.37805 -0.45665 -0.60386 -0.41089 0.078006 -1.3394 0.049803 -0.91783 -0.47655 0.79018 -0.28336\\n',\n",
       " 'not -0.19104 0.17601 0.3692 -0.50323 -0.47561 0.15798 -0.11679 0.21052 0.32652 0.12194 0.090944 0.26089 0.76294 0.00069673 -0.050001 -0.44853 0.36239 0.56345 -0.68702 0.33237 0.31285 -0.14207 0.35327 -0.16426 -0.10693 0.077786 -0.17704 -0.92897 0.1468 -0.13585 0.25682 0.66019 -0.35569 0.21838 0.38173 0.54337 0.10197 0.3523 -0.2551 -0.15155 -0.67434 0.16903 0.16413 -0.53843 -0.17457 -0.28539 0.74044 -0.67533 -0.23382 -1.3599 0.30225 -0.14968 0.27043 1.1979 -0.29556 -2.5395 0.0010303 -0.26272 1.8303 0.80008 -0.35691 0.56578 -0.5504 0.070845 1.4275 0.09016 0.7842 0.7849 -0.33538 -0.65751 -0.20112 -1.0297 0.069195 -0.61272 0.11373 -0.19547 -0.21256 0.049763 -1.1619 -0.064512 0.53146 -0.47384 -0.68709 0.13024 -2.0899 -0.41346 0.30364 -0.00057448 -0.18833 -0.54779 -0.32058 -0.36704 -0.1474 -0.19044 -0.47712 0.048228 -0.26215 -0.5968 0.080843 0.27866\\n',\n",
       " 'this -0.57058 0.44183 0.70102 -0.41713 -0.34058 0.02339 -0.071537 0.48177 -0.013121 0.16834 -0.13389 0.040626 0.15827 -0.44342 -0.019403 -0.009661 -0.046284 0.093228 -0.27331 0.2285 0.33089 -0.36474 0.078741 0.3585 0.44757 -0.2299 0.18077 -0.6265 0.053852 -0.29154 -0.4256 0.62903 0.14393 -0.046004 -0.21007 0.48879 -0.057698 0.37431 -0.030075 -0.34494 -0.29702 0.15095 0.28248 -0.16578 0.076131 -0.093016 0.79365 -0.60489 -0.18874 -1.0173 0.31962 -0.16344 0.54177 1.1725 -0.47875 -3.3842 -0.081301 -0.3528 1.8372 0.44516 -0.52666 0.99786 -0.32178 0.033462 1.1783 -0.072905 0.39737 0.26166 0.33111 -0.35629 -0.16558 -0.44382 -0.14183 -0.37976 0.28994 -0.029114 -0.35169 -0.27694 -1.344 0.19555 0.16887 0.040237 -0.80212 0.23366 -1.3837 -0.023132 0.085395 -0.74051 -0.073934 -0.58838 -0.085735 -0.10525 -0.51571 0.15038 -0.16694 -0.16372 -0.22702 -0.66102 0.47197 0.37253\\n',\n",
       " 'who 0.26164 0.4472 -0.096845 -0.74067 0.20805 0.33434 -0.26796 0.022865 -0.37251 0.22637 -0.22139 0.20357 0.34547 0.17839 0.15189 -0.09791 0.95879 0.34033 -1.0375 0.33589 0.29997 0.33378 0.56341 -0.04013 0.27697 -0.56576 -0.37009 -0.74752 0.62616 -0.54981 0.26671 1.3877 -0.17649 0.40599 0.48077 0.18777 -0.25752 1.0871 0.55904 0.12204 -1.4921 0.17677 0.90724 -0.7636 0.1091 -0.024877 -0.14734 -0.55559 0.74541 -0.59346 -0.19779 -0.42991 0.30361 1.2408 -0.19161 -2.0891 -0.2374 -0.33597 0.60638 1.4537 0.46323 0.53161 0.067068 -0.043899 0.7305 -0.20189 1.0433 0.55102 -0.25149 0.45955 -0.043735 -0.70793 -0.70816 -0.38365 -0.28454 -0.24853 -0.45679 -0.21562 -0.87828 -0.55946 1.2574 0.37382 -0.84276 0.015869 -1.8082 -0.87081 -0.37524 0.033488 -0.46289 -0.9618 0.28619 -0.58053 0.46974 -0.089393 -1.1858 0.3699 -0.58993 -0.4503 0.49525 -0.20298\\n',\n",
       " 'they -0.07954 0.30171 0.079516 -0.74662 -0.67879 0.35029 -0.19754 0.4929 0.14162 -0.23789 0.10939 0.23465 0.77763 0.12745 0.10873 -0.68024 0.25696 0.53981 -0.92294 0.088309 0.5524 0.073341 0.63424 -0.094834 -0.068988 -0.11287 -0.1932 -0.61233 0.16718 -0.43107 0.29355 0.42588 -0.22194 0.14787 0.53693 0.12846 0.12732 0.50899 0.2408 -0.3513 -0.52486 -0.37477 -0.084382 -0.39593 -0.14876 -0.030951 0.48431 -0.24678 0.12347 -1.1037 -0.09493 -0.038439 0.1075 1.6517 -0.10342 -2.4332 0.040486 -0.39164 1.5943 0.95891 -0.52496 1.1476 -0.57502 0.044767 1.097 -0.15884 0.59743 0.66712 -0.095168 -0.56173 -0.067523 -0.79894 0.066405 -0.82591 -0.58701 0.18065 -0.28443 -0.091645 -1.0481 -0.051988 0.75936 -0.039712 -0.9376 0.056433 -2.1626 -0.64355 0.42302 -0.10572 -0.72611 -0.1294 -0.38103 -0.7459 -0.0096764 -0.16205 -1.0562 -0.11999 -0.60909 -0.16731 0.65344 0.366\\n',\n",
       " 'had 0.63256 -0.12718 -0.084182 -0.30718 -0.2526 -0.16172 0.47123 0.46553 -0.051526 0.17231 0.42743 0.57854 0.64548 0.31367 0.32752 -0.47608 0.25888 0.035845 -0.95154 -0.20671 0.65171 0.010712 0.3894 0.069552 0.061198 -0.72152 -0.22334 -0.34747 -0.1434 -0.32482 0.79539 0.84708 -0.046052 0.74384 0.18185 0.18666 0.17123 0.93485 -0.1299 0.39219 -1.016 -0.27859 0.79293 -0.40433 0.41505 0.017283 0.2936 -0.72944 0.98233 -0.96504 -0.19016 -0.010012 0.26765 1.3316 -0.19376 -2.4401 -0.39477 -0.36232 0.88315 1.5974 0.31113 0.72201 -0.55023 0.54995 0.6846 -0.40378 0.52677 0.23314 -0.12764 0.12305 -0.12489 -0.25052 0.11632 -0.41648 -0.19738 0.093786 -0.24532 0.29676 -1.6156 0.38933 0.68466 0.048371 -0.4177 0.10663 -2.0738 -0.81679 0.56651 -0.32536 -0.76685 -0.27123 -0.10798 -0.61763 -0.27098 0.077119 -0.89352 0.17811 -0.50156 -0.30966 0.22378 0.038183\\n',\n",
       " 'i -0.046539 0.61966 0.56647 -0.46584 -1.189 0.44599 0.066035 0.3191 0.14679 -0.22119 0.79239 0.29905 0.16073 0.025324 0.18678 -0.31001 -0.28108 0.60515 -1.0654 0.52476 0.064152 1.0358 -0.40779 -0.38011 0.30801 0.59964 -0.26991 -0.76035 0.94222 -0.46919 -0.18278 0.90652 0.79671 0.24825 0.25713 0.6232 -0.44768 0.65357 0.76902 -0.51229 -0.44333 -0.21867 0.3837 -1.1483 -0.94398 -0.15062 0.30012 -0.57806 0.20175 -1.6591 -0.079195 0.026423 0.22051 0.99714 -0.57539 -2.7266 0.31448 0.70522 1.4381 0.99126 0.13976 1.3474 -1.1753 0.0039503 1.0298 0.064637 0.90887 0.82872 -0.47003 -0.10575 0.5916 -0.4221 0.57331 -0.54114 0.10768 0.39784 -0.048744 0.064596 -0.61437 -0.286 0.5067 -0.49758 -0.8157 0.16408 -1.963 -0.26693 -0.37593 -0.95847 -0.8584 -0.71577 -0.32343 -0.43121 0.41392 0.28374 -0.70931 0.15003 -0.2154 -0.37616 -0.032502 0.8062\\n',\n",
       " 'which 0.03024 0.44606 0.43166 -0.37528 0.29068 0.23032 0.18125 0.40201 0.13518 -0.19562 0.30639 -0.13239 0.67897 0.42234 0.32637 -0.15281 0.37698 -0.23303 -0.33817 0.30588 0.44918 -0.83624 0.59146 0.24958 0.39986 -0.50172 -0.23544 -0.14696 -0.35144 -0.56852 0.08954 0.82612 -0.26586 0.3903 -0.036849 0.48257 0.71664 0.11004 -0.59354 -0.33216 -0.25736 -0.34531 -0.026326 -0.23747 0.00019656 -0.2748 0.38512 -0.39581 0.11404 -0.25174 -0.3247 0.089608 0.24929 1.5127 -0.19762 -2.8509 -0.53833 -0.47111 1.7859 0.78126 -0.12963 0.56077 0.32151 0.35571 0.84547 0.14931 0.11487 0.30625 0.54774 -0.50426 0.33824 -0.62043 -0.012869 0.066666 0.062731 -0.44534 0.15541 0.21801 -1.732 0.42054 0.36319 -0.072258 -0.74811 0.19888 -1.4461 -0.27576 0.26646 -0.57838 0.56151 -0.028701 -0.2466 -0.425 -0.57154 0.31939 -0.22075 0.46528 -0.16606 -0.79923 0.80849 0.37378\\n',\n",
       " 'will -0.26703 0.44911 0.55478 -0.69003 0.046175 -0.43044 -0.29348 1.0149 -0.33757 -0.096388 -0.28176 0.41828 0.58357 -0.078788 -0.23511 -0.74174 0.68242 0.77152 -0.80698 0.13537 0.19157 -0.51766 0.30764 0.68624 0.15603 -0.13725 0.064215 -0.25319 0.33305 -0.61336 -0.60918 0.72316 -0.29642 0.34352 0.36234 0.74162 0.30484 0.41837 -0.35164 -0.41675 -0.32865 -0.56729 -0.11949 -0.70703 -0.20916 0.050621 0.057078 -0.56363 0.032941 -1.1897 0.0467 -0.48235 -0.12548 1.088 0.095233 -2.5264 -0.15569 -0.081787 1.9323 0.75413 -0.21121 0.7779 -0.41152 -0.019988 0.83285 0.45991 0.19578 1.1235 -0.21463 -1.0439 0.64653 -1.0831 -0.47316 -0.76509 0.22128 -0.38609 -0.24266 0.28377 -0.8266 -0.022102 0.9439 -0.40069 -0.65322 -0.053926 -1.6434 -0.57158 0.50725 0.24536 0.3272 -0.53953 -0.12521 0.071234 -0.14235 -0.41365 -0.68288 0.22851 0.56056 -0.87247 0.83828 0.465\\n',\n",
       " 'their 0.17137 -0.33437 0.11471 -0.52008 -0.79816 0.48592 -0.73885 0.28653 -0.6273 0.26786 -0.32081 0.11757 0.61241 0.25964 -0.21165 -0.63307 0.40135 0.26175 -0.16827 -0.0092904 0.35687 -0.31246 0.79867 0.54921 0.17423 -0.30704 -0.23997 -0.64167 0.36679 -0.27327 0.64719 0.32952 -0.46198 0.057414 -0.088337 -0.027794 -0.074753 0.52992 -0.48819 -0.17822 0.045592 -0.39832 0.5287 0.01556 0.23284 -0.011029 -0.15533 -0.065942 0.22879 -1.0154 -0.2172 0.26772 -0.32618 1.6999 0.055699 -2.586 0.75177 -0.76514 1.9965 0.28489 -0.1825 1.3012 -0.074158 0.22211 0.8391 0.12388 0.4363 0.4045 0.25744 -0.63188 -0.34572 -0.097293 0.14742 -1.5755 -0.098063 0.36815 -0.83086 -0.41775 -1.3509 0.11379 0.76722 0.75707 -0.24826 0.4637 -1.8673 -0.46703 -0.01854 0.010588 -0.76759 0.21896 -0.46664 0.32206 0.2232 0.30753 -1.179 -0.85791 -1.0137 -0.11272 1.1948 0.53682\\n',\n",
       " ': -0.54558 1.0965 1.5106 -0.4727 -0.15547 0.21748 -0.36125 0.12727 -0.51148 -0.35528 0.59787 -0.53174 -0.046401 0.65336 0.10376 0.30513 0.38383 -0.22857 -0.78904 1.0066 0.20339 -0.0020182 0.21937 0.64771 0.40531 0.58813 -0.022486 -0.43952 0.15482 -0.48849 -0.60316 0.36198 -0.45655 -0.51369 -0.28462 0.19641 0.42976 0.46083 -0.22081 -0.34077 -0.21216 -0.36246 -0.1494 -0.66431 -0.13962 -0.25126 -0.4993 -0.29225 -0.36749 -0.42177 0.35742 0.28964 -0.14963 1.3742 -0.80804 -2.7821 0.12325 0.41852 1.0374 0.50126 -0.98822 0.76788 -0.30979 -0.55975 1.0845 -0.18401 0.26683 0.39754 0.22833 0.43425 -0.12324 0.21173 -0.40471 -0.44548 0.27367 -0.1379 0.69137 0.32499 -1.361 -0.9774 0.48218 -0.76722 -0.40092 -0.2129 -0.82015 -0.039415 -0.37735 -0.57144 0.070427 -0.091096 0.016801 0.38154 0.39406 0.38044 -0.58943 -0.021365 0.17364 -0.90374 0.48136 0.030378\\n',\n",
       " 'or 0.31039 0.64859 0.28481 -0.46756 -0.25715 0.71389 -0.064742 0.30187 0.52801 0.19849 0.20011 -0.072665 0.23712 0.72137 0.65764 -0.42129 0.18458 -0.11517 -0.4138 0.54855 0.13857 -0.090728 0.3171 0.26416 0.53625 0.068179 -0.096772 -0.64557 0.12414 -0.28334 0.39239 0.57338 -0.58856 -0.44752 0.22755 0.61385 0.07508 -0.11876 0.070905 0.12847 -0.40008 -0.66973 -0.24658 -0.69349 -0.20067 -0.1633 -0.22338 -0.032656 -0.52169 -1.4363 0.068198 0.27494 0.39587 1.3418 -0.66544 -1.9608 0.27415 -0.13552 2.252 0.7521 -0.28896 0.83412 -0.29035 0.291 0.81412 -0.38679 0.71252 -0.29467 0.5797 -0.31411 -1.0094 -0.32144 -0.033035 -0.46199 -0.051451 0.057455 -0.16413 0.28121 -0.58346 0.11114 0.2778 -0.13717 -0.66281 0.45848 -1.6707 0.36448 0.31026 0.48856 0.88709 -0.20468 -0.62125 -0.021678 0.14065 0.10319 -0.74186 -0.03031 -0.25264 -0.88554 0.91767 -0.57253\\n',\n",
       " 'its 0.20589 -0.3171 0.74431 0.047407 -0.10826 -0.18517 -0.47992 0.27736 -0.47515 0.63156 0.035472 0.073577 0.36199 -0.1923 -0.43884 -0.32988 0.44746 -0.39097 0.46493 0.12546 0.38224 -0.45183 0.29384 0.62979 0.32395 -0.22369 -0.16764 -0.30502 -0.10406 -0.27813 0.32921 0.62754 -0.57658 0.12239 -0.56023 0.44731 0.48498 -0.20299 -0.92786 -0.22441 0.21687 -0.26173 0.50944 0.30419 0.5882 0.40474 0.57113 -0.24145 -0.076213 -0.61215 -0.016607 0.10699 -0.083368 1.4618 -0.58467 -3.2127 0.41215 -0.79032 2.7901 -0.39825 0.11872 0.58777 0.25648 0.42427 0.63226 -0.1714 0.11396 0.61377 0.65834 -0.21013 0.24094 -0.036859 -0.2394 -0.87062 0.2499 -0.55732 -0.494 -0.12043 -1.6417 0.43463 0.29555 0.075822 -0.3107 1.0174 -1.115 -0.31493 0.41304 -0.1616 0.070745 0.21949 0.034177 0.78235 -0.39665 0.34179 -0.73382 -0.45151 -0.81274 -0.67858 1.2779 0.86475\\n',\n",
       " 'one -0.22557 0.49418 0.4861 -0.4332 0.13738 0.50617 0.26058 0.30103 -0.091486 0.10876 0.3058 0.051028 0.22303 0.054236 0.068838 -0.24701 0.32689 -0.082203 -0.28866 0.3734 0.73804 -0.040969 0.040201 0.11384 0.69987 -0.49745 -0.06755 -0.42599 -0.10725 -0.010697 -0.01479 0.55976 0.3064 0.053053 0.058034 0.32756 -0.37233 0.46513 0.14285 -0.085003 -0.45476 0.19773 0.6383 -0.31148 0.10858 0.31557 0.36682 -0.35135 -0.48414 -0.33235 -0.33816 -0.39678 0.1908 1.3513 -0.39044 -2.8795 -0.14276 -0.087754 1.7713 0.99332 -0.14129 0.94389 0.050897 0.47373 0.86387 -0.16162 0.67199 0.52344 0.14438 -0.055194 -0.34669 -0.20742 0.18907 -0.19845 0.34862 0.10121 -0.092119 -0.66258 -1.0582 -0.11803 0.70171 0.077776 -0.50546 0.032243 -1.6176 -0.29302 -0.061748 -0.32473 0.3439 -0.44698 0.085689 0.13295 -0.1807 -0.11854 -0.82985 0.13784 -0.34359 -0.45744 0.49646 0.34906\\n',\n",
       " 'after 0.37711 -0.34471 0.13405 -0.01171 -0.19427 0.41464 0.40608 0.43063 -0.05706 -0.19921 0.43267 -0.016269 0.2171 -0.0026149 0.39424 -0.42803 -0.017495 -0.56658 -0.44558 -0.18529 0.26732 -0.15712 0.21657 0.79714 0.69623 0.20405 -0.49907 -0.45519 0.3821 0.20603 -0.21606 0.10093 -0.50148 -0.11058 -0.43455 -0.26785 -0.20234 0.003832 -0.49108 -0.17642 -0.88971 -0.279 0.86387 -0.017356 0.3121 0.41004 0.23199 -0.60812 0.44763 -0.89579 -0.038491 -0.25772 0.39468 1.6186 -0.54882 -3.0291 -0.77845 -0.32463 1.7658 0.97303 -0.39342 0.54811 0.013164 0.3785 0.24538 0.031079 0.23628 0.28901 0.027047 0.28985 -0.74523 0.011517 -0.39456 -0.57706 -0.63604 0.31022 -0.38317 -0.077663 -1.3539 0.018009 0.85646 0.038259 -0.39437 0.44331 -1.0802 -0.43159 0.14391 0.11854 -0.56459 -0.47966 0.2286 -0.24369 -0.42823 1.0366 -0.83071 0.1246 0.2063 0.54232 0.11425 -0.66927\\n']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[:50]\n",
    "# here we can see that against each word there is word vector of 100 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1f758f0-bb7d-4ff0-8204-360a591e2ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are going to create a dictionary that will map the word to their \n",
    "# corresponding vector\n",
    "embedding = {}\n",
    "\n",
    "# lets iterate through each line;\n",
    "for line in content:\n",
    "    line = line.split()\n",
    "    \n",
    "    # we are going to map the first word against the remaining vector\n",
    "    embedding[line[0]] = np.array(line[1:], dtype=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e112bd5-c081-46fb-abe9-dee33671ce03",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding.keys()) # here we can see that we have 400000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "060d1f67-2192-4b83-b77f-05feaa610ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.preprocessing.text.Tokenizer at 0x1a1a8bd5220>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# next thing we have to preprocess the data - converting the data into input \n",
    "# tokens and convert y into one-hot vector\n",
    "# for tokening the input text, a tokenizer function from keras will be used\n",
    "# it will convert each specific word from text into a number and number is \n",
    "# assigned to a order in which that word is appearing in the dictionary\n",
    "# lets initiate the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "671d10d2-174d-4faa-b8c9-a2d386302d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets fit this onto our dataset\n",
    "tokenizer.fit_on_texts(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7121d217-a276-4db8-b1b0-83420802c63b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets see the output of above line\n",
    "word_to_index = tokenizer.word_index\n",
    "len(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78f16744-eb64-4b12-ab06-1f982a05c9c4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[103, 104, 3, 6, 105],\n",
       " [106, 3, 107],\n",
       " [1, 7, 108],\n",
       " [109, 4, 35],\n",
       " [36, 30],\n",
       " [37, 3, 19, 110, 26, 49],\n",
       " [1, 111, 112],\n",
       " [31, 67, 113],\n",
       " [1, 20, 114, 27],\n",
       " [115, 68, 38, 69, 26],\n",
       " [2, 11, 116, 10, 70],\n",
       " [117, 50, 71, 51],\n",
       " [36, 39],\n",
       " [12, 12, 12, 22, 28, 6, 40],\n",
       " [1, 32, 21, 5, 118, 119],\n",
       " [120, 11, 2, 121, 41],\n",
       " [1, 20, 9, 30],\n",
       " [1, 72, 52, 53, 13, 10],\n",
       " [4, 122, 3, 123],\n",
       " [73, 3, 4, 35],\n",
       " [1, 7, 124],\n",
       " [12, 12, 12, 54],\n",
       " [14, 52, 53],\n",
       " [15, 23, 5, 125],\n",
       " [126, 3, 127, 1, 21],\n",
       " [15, 74, 128, 129, 75],\n",
       " [1, 18, 9, 4, 130, 55, 131],\n",
       " [29, 2, 24, 132],\n",
       " [2, 133, 134, 10, 135],\n",
       " [1, 33, 2, 6, 76],\n",
       " [1, 24, 19, 136],\n",
       " [14, 23, 16, 5, 137],\n",
       " [32, 2, 138, 8, 77],\n",
       " [2, 139, 4, 140],\n",
       " [141, 3, 56, 13, 5, 78],\n",
       " [77, 42, 3, 142],\n",
       " [43, 6, 79],\n",
       " [29, 2, 21, 5, 35],\n",
       " [15, 74, 17, 29, 143],\n",
       " [15, 80, 26],\n",
       " [50, 57, 81, 5, 144, 44, 145],\n",
       " [146, 11, 147],\n",
       " [15, 3, 5, 36, 148],\n",
       " [72, 149, 9, 16, 51],\n",
       " [1, 33, 58],\n",
       " [27, 3, 34],\n",
       " [1, 7, 150, 59],\n",
       " [6, 41, 45, 2, 151, 152, 46, 68],\n",
       " [29, 2, 20, 9, 153, 16, 13, 44],\n",
       " [1, 24, 9, 60],\n",
       " [15, 25, 154, 155, 39],\n",
       " [31, 82, 47, 16],\n",
       " [1, 18, 156, 157],\n",
       " [2, 11, 158, 159, 55, 160],\n",
       " [1, 7, 83, 48, 19, 161],\n",
       " [6, 162, 2, 11, 17, 163],\n",
       " [40],\n",
       " [31, 164, 165],\n",
       " [84, 13, 4, 166, 22, 23, 5, 61, 48, 167],\n",
       " [10, 168, 3, 43],\n",
       " [1, 28, 169, 13, 58, 13, 85, 170],\n",
       " [14, 171, 86, 9, 172, 173],\n",
       " [8, 174, 3, 4, 18, 48, 8, 34],\n",
       " [1, 32, 175, 176],\n",
       " [8, 177, 3, 62, 178, 4, 87, 179, 16, 180],\n",
       " [14, 3, 4, 181, 182, 1, 21, 183, 184],\n",
       " [15, 3, 185],\n",
       " [1, 186, 8, 88],\n",
       " [1, 18, 2, 187],\n",
       " [43, 39],\n",
       " [188, 189, 2, 190, 45],\n",
       " [10, 191, 28, 81, 5, 30],\n",
       " [1, 18, 192, 27],\n",
       " [11, 2, 56, 13, 26, 10, 193],\n",
       " [10, 3, 41],\n",
       " [19, 194, 195, 89, 196],\n",
       " [1, 18, 8, 197],\n",
       " [29, 2, 20, 9, 198, 16, 5, 199],\n",
       " [10, 90, 28, 200],\n",
       " [1, 7, 91],\n",
       " [1, 33, 63],\n",
       " [37, 3, 201, 46, 2],\n",
       " [202, 11, 6, 203, 55, 204],\n",
       " [1, 7, 6, 205, 92, 19, 206, 9, 10, 207],\n",
       " [50, 208, 22],\n",
       " [1, 7, 209, 27],\n",
       " [210, 24, 5, 59, 211, 12, 12],\n",
       " [1, 7, 6, 93, 13, 2],\n",
       " [33, 2, 6, 76],\n",
       " [1, 18, 2],\n",
       " [10, 30, 3, 212, 16, 213],\n",
       " [2, 11, 17, 214, 13, 10, 215],\n",
       " [33, 2, 8, 216],\n",
       " [1, 20, 9, 94],\n",
       " [1, 7, 6, 91, 9, 95, 2, 217, 6, 96],\n",
       " [15, 3, 4, 218, 219],\n",
       " [37, 5, 59, 220],\n",
       " [8, 221, 222, 223],\n",
       " [31, 82, 47, 16],\n",
       " [58, 60, 3, 6, 224],\n",
       " [22, 3, 4, 225, 42, 226, 8, 34],\n",
       " [15, 3, 227],\n",
       " [89, 228, 80, 63],\n",
       " [14, 3, 229],\n",
       " [22, 28, 40, 54],\n",
       " [15, 3, 6, 64],\n",
       " [2, 25, 75, 65, 2, 230],\n",
       " [1, 231, 1, 32, 232, 233, 234],\n",
       " [66, 21, 27, 235],\n",
       " [97, 41, 45, 2, 236, 17, 237],\n",
       " [1, 20, 9, 38, 69],\n",
       " [2, 11, 5, 238],\n",
       " [1, 7, 239],\n",
       " [2, 240],\n",
       " [98],\n",
       " [2, 241, 17, 242, 22],\n",
       " [1, 71, 8, 243],\n",
       " [14, 25, 17, 99, 8, 244],\n",
       " [45, 245, 246],\n",
       " [95, 2, 47, 4, 78],\n",
       " [1, 247, 248],\n",
       " [1, 52, 53],\n",
       " [249, 3, 34],\n",
       " [4, 49, 100, 250],\n",
       " [4, 251, 252, 253, 23, 4, 35],\n",
       " [98, 65, 19, 254],\n",
       " [4, 255, 3, 97, 96],\n",
       " [54],\n",
       " [1, 23, 256, 92, 8, 257],\n",
       " [1, 20, 9, 94],\n",
       " [4, 258, 11, 43, 259],\n",
       " [2, 25, 17, 29, 19, 260],\n",
       " [4, 101, 3, 261],\n",
       " [84],\n",
       " [1, 262, 2],\n",
       " [1, 7, 263, 13, 5, 264],\n",
       " [73, 3, 4, 27],\n",
       " [2, 11, 265],\n",
       " [266, 267, 13, 44],\n",
       " [14, 3, 93],\n",
       " [1, 7, 268, 62],\n",
       " [10, 3, 6, 40],\n",
       " [2, 23, 5, 56, 269],\n",
       " [1, 20, 9, 21, 270, 13, 44],\n",
       " [14, 271, 5, 61],\n",
       " [4, 272, 273, 274, 51],\n",
       " [1, 23, 275],\n",
       " [276, 11, 36],\n",
       " [1, 277, 63],\n",
       " [1, 7, 278, 9, 4, 279],\n",
       " [1, 7, 280, 281],\n",
       " [1, 7, 83, 48, 2, 86],\n",
       " [10, 90, 3, 67, 46, 16],\n",
       " [282, 65, 4, 283, 39],\n",
       " [284, 19, 285],\n",
       " [38, 286],\n",
       " [1, 287, 288, 8, 289],\n",
       " [290, 291, 21, 5, 101],\n",
       " [1, 7, 292],\n",
       " [14, 3, 8, 293, 18],\n",
       " [14, 3, 6, 64],\n",
       " [1, 18, 88],\n",
       " [1, 25, 17, 21, 294],\n",
       " [8, 295, 100, 57, 5, 296, 297],\n",
       " [1, 24, 2, 5, 61],\n",
       " [15, 57, 9, 298, 5, 299, 300],\n",
       " [1, 7, 47, 4, 26, 49],\n",
       " [11, 2, 301, 12, 12],\n",
       " [1, 24, 9, 302],\n",
       " [31, 303, 10, 30, 12, 12, 12],\n",
       " [2, 85, 11, 64],\n",
       " [10, 304, 87, 3, 17, 62],\n",
       " [37, 2, 25, 28, 79],\n",
       " [8, 34, 3, 6, 305],\n",
       " [15, 25, 17, 99],\n",
       " [66, 70],\n",
       " [2, 306, 8, 42],\n",
       " [1, 32, 38, 102],\n",
       " [66, 307, 308, 42],\n",
       " [102, 46, 16],\n",
       " [14, 3, 5, 309],\n",
       " [14, 310, 26],\n",
       " [1, 24, 22, 311, 312, 60]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# next all words will be converted into a list of tokens\n",
    "x_tokens = tokenizer.texts_to_sequences(x)\n",
    "x_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee19c801-4fd9-48d8-97d2-80a48e4731b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we cann see that all tokens have arbitrary length -  this cannot be used \n",
    "# directly in the model. we need a fixed length input\n",
    "# we will be using padding to transfer it into a fixed length vector - the fixed\n",
    "# length will be the largest vector of this dataset\n",
    "maxlen = 0\n",
    "for data in x_tokens:\n",
    "    maxlen = max(maxlen, len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cea68f98-9743-4271-8a7c-652cb2bec13e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen # the max length of our sentence here is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29d03228-38f0-42b4-99df-e5445bc9d403",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[103, 104,   3,   6, 105,   0,   0,   0,   0,   0],\n",
       "       [106,   3, 107,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  1,   7, 108,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [109,   4,  35,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [ 36,  30,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [ 37,   3,  19, 110,  26,  49,   0,   0,   0,   0],\n",
       "       [  1, 111, 112,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [ 31,  67, 113,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  1,  20, 114,  27,   0,   0,   0,   0,   0,   0],\n",
       "       [115,  68,  38,  69,  26,   0,   0,   0,   0,   0],\n",
       "       [  2,  11, 116,  10,  70,   0,   0,   0,   0,   0],\n",
       "       [117,  50,  71,  51,   0,   0,   0,   0,   0,   0],\n",
       "       [ 36,  39,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [ 12,  12,  12,  22,  28,   6,  40,   0,   0,   0],\n",
       "       [  1,  32,  21,   5, 118, 119,   0,   0,   0,   0],\n",
       "       [120,  11,   2, 121,  41,   0,   0,   0,   0,   0],\n",
       "       [  1,  20,   9,  30,   0,   0,   0,   0,   0,   0],\n",
       "       [  1,  72,  52,  53,  13,  10,   0,   0,   0,   0],\n",
       "       [  4, 122,   3, 123,   0,   0,   0,   0,   0,   0],\n",
       "       [ 73,   3,   4,  35,   0,   0,   0,   0,   0,   0],\n",
       "       [  1,   7, 124,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [ 12,  12,  12,  54,   0,   0,   0,   0,   0,   0],\n",
       "       [ 14,  52,  53,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [ 15,  23,   5, 125,   0,   0,   0,   0,   0,   0],\n",
       "       [126,   3, 127,   1,  21,   0,   0,   0,   0,   0],\n",
       "       [ 15,  74, 128, 129,  75,   0,   0,   0,   0,   0],\n",
       "       [  1,  18,   9,   4, 130,  55, 131,   0,   0,   0],\n",
       "       [ 29,   2,  24, 132,   0,   0,   0,   0,   0,   0],\n",
       "       [  2, 133, 134,  10, 135,   0,   0,   0,   0,   0],\n",
       "       [  1,  33,   2,   6,  76,   0,   0,   0,   0,   0],\n",
       "       [  1,  24,  19, 136,   0,   0,   0,   0,   0,   0],\n",
       "       [ 14,  23,  16,   5, 137,   0,   0,   0,   0,   0],\n",
       "       [ 32,   2, 138,   8,  77,   0,   0,   0,   0,   0],\n",
       "       [  2, 139,   4, 140,   0,   0,   0,   0,   0,   0],\n",
       "       [141,   3,  56,  13,   5,  78,   0,   0,   0,   0],\n",
       "       [ 77,  42,   3, 142,   0,   0,   0,   0,   0,   0],\n",
       "       [ 43,   6,  79,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [ 29,   2,  21,   5,  35,   0,   0,   0,   0,   0],\n",
       "       [ 15,  74,  17,  29, 143,   0,   0,   0,   0,   0],\n",
       "       [ 15,  80,  26,   0,   0,   0,   0,   0,   0,   0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets padd the x_tokens and called it training data\n",
    "x_train = pad_sequences(x_tokens, maxlen=maxlen, padding='post', truncating='post')\n",
    "x_train[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "856997e9-3be4-4578-9e5a-4afca8fe78bf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['4', '3', '3', '1', '2', '1', '4', '3', '4', '1', '3', '3', '2',\n",
       "       '2', '4', '3', '2', '3', '3', '1', '3', '2', '2', '2', '0', '1',\n",
       "       '0', '4', '2', '0', '2', '0', '0', '3', '4', '0', '2', '1', '3',\n",
       "       '1', '0', '4', '0', '3', '0', '4', '2', '3', '4', '2', '2', '3',\n",
       "       '0', '2', '2', '3', '2', '3', '2', '2', '3', '3', '0', '2', '3',\n",
       "       '0', '2', '0', '0', '2', '3', '2', '4', '1', '3', '3', '0', '0',\n",
       "       '3', '2', '0', '3', '0', '2', '2', '4', '2', '2', '0', '0', '2',\n",
       "       '3', '0', '4', '2', '1', '2', '3', '3', '2', '3', '0', '3', '0',\n",
       "       '2', '0', '2', '3', '4', '3', '1', '3', '4', '3', '2', '3', '3',\n",
       "       '3', '1', '4', '4', '2', '2', '1', '1', '2', '3', '2', '3', '4',\n",
       "       '2', '3', '0', '2', '0', '0', '4', '3', '4', '2', '3', '2', '3',\n",
       "       '4', '2', '1', '2', '4', '3', '1', '3', '2', '3', '2', '2', '3',\n",
       "       '3', '2', '4', '0', '0', '0', '3', '0', '0', '1', '1', '2', '2',\n",
       "       '2', '0', '3', '2', '3', '3', '1', '2', '2', '4', '2', '3', '1',\n",
       "       '2'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the x_data is ready to be used for training\n",
    "# next convert the y into one-hot encoding\n",
    "# as our data contain some values which we  don't need, we need to clean the\n",
    "# data\n",
    "y_ = [(lambda x: x.split()[0][0]) (x) for x in y.tolist()]\n",
    "y = np.array(y_, dtype=object)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7aab00ee-240d-4e60-9b8b-892eeb99a068",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets convert it into one-hot, for that, we will be using a builtin \n",
    "# function to_categorical\n",
    "y_train = to_categorical(y)\n",
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f03773-c08f-4bca-8985-29927a11b4b8",
   "metadata": {},
   "source": [
    "# Model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42964fb9-6596-4898-bf64-993342b27f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are going to use pretrained word embeddings - for that we will pass \n",
    "# matrix to weights parameters in the models. Lets create that matrix\n",
    "# here that matrix will have all the words embedding vectors of all the words in\n",
    "# our dataset\n",
    "# here the rows will be the words and the columns will be the vectors\n",
    "embed_size = 100 # as we are using 100 dimension vector\n",
    "\n",
    "# lets initialize the matrix will zero values\n",
    "embedding_matrix = np.zeros((len(word_to_index)+1, embed_size))\n",
    "\n",
    "# lets iterate over all the words in our embedding dict\n",
    "for word, index in word_to_index.items():\n",
    "    embed_vector = embedding[word]\n",
    "    embedding_matrix[index] = embed_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10d3e1c0-ea82-46b8-85bf-23d6428d9ef7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "         0.      ],\n",
       "       [-0.046539,  0.61966 ,  0.56647 , ..., -0.37616 , -0.032502,\n",
       "         0.8062  ],\n",
       "       [-0.49886 ,  0.76602 ,  0.89751 , ..., -0.41179 ,  0.40539 ,\n",
       "         0.78504 ],\n",
       "       ...,\n",
       "       [-0.46263 ,  0.069864,  0.69095 , ..., -0.29174 ,  0.32041 ,\n",
       "         0.21202 ],\n",
       "       [ 0.073242,  0.11134 ,  0.62281 , ...,  0.53417 , -0.1646  ,\n",
       "        -0.27516 ],\n",
       "       [ 0.29019 ,  0.80497 ,  0.31187 , ..., -0.33603 ,  0.45998 ,\n",
       "        -0.11278 ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f2d21a3-73f4-4024-8664-dfa2d49a493d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create the model now\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=len(word_to_index)+1,\n",
    "             output_dim=embed_size,\n",
    "             input_length=maxlen,\n",
    "              weights=[embedding_matrix],\n",
    "              trainable=False\n",
    "             ),\n",
    "    # next we can specify simple LSTM or RNN layer - first LSTM\n",
    "    LSTM(units=16, return_sequences=True), # return_sequence will make sure to \n",
    "                                            # return the value at each sequence\n",
    "     LSTM(units=10, return_sequences=True),\n",
    "    LSTM(units=4), # we do not have to specify the return_sequence at last layer\n",
    "    Dense(units=5, activation='softmax'),\n",
    "    \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e5165ca-dc8d-475e-8dff-c3d7aff10365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 10, 100)           31300     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 10, 16)            7488      \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 10, 10)            1080      \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 4)                 240       \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 25        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,133\n",
      "Trainable params: 8,833\n",
      "Non-trainable params: 31,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8964706-9564-4a2d-87ca-9a6750430f06",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "6/6 [==============================] - 9s 19ms/step - loss: 1.6108 - accuracy: 0.1913\n",
      "Epoch 2/50\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 1.5875 - accuracy: 0.2459\n",
      "Epoch 3/50\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 1.5719 - accuracy: 0.2678\n",
      "Epoch 4/50\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 1.5567 - accuracy: 0.2896\n",
      "Epoch 5/50\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 1.5403 - accuracy: 0.2678\n",
      "Epoch 6/50\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 1.5243 - accuracy: 0.3333\n",
      "Epoch 7/50\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 1.5055 - accuracy: 0.3497\n",
      "Epoch 8/50\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 1.4843 - accuracy: 0.3497\n",
      "Epoch 9/50\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 1.4588 - accuracy: 0.3716\n",
      "Epoch 10/50\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 1.4304 - accuracy: 0.4044\n",
      "Epoch 11/50\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 1.3989 - accuracy: 0.4208\n",
      "Epoch 12/50\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.3661 - accuracy: 0.4317\n",
      "Epoch 13/50\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.3374 - accuracy: 0.4536\n",
      "Epoch 14/50\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.3065 - accuracy: 0.4973\n",
      "Epoch 15/50\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.2664 - accuracy: 0.5410\n",
      "Epoch 16/50\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 1.2356 - accuracy: 0.5519\n",
      "Epoch 17/50\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.2067 - accuracy: 0.5628\n",
      "Epoch 18/50\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.1855 - accuracy: 0.5574\n",
      "Epoch 19/50\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 1.1749 - accuracy: 0.5847\n",
      "Epoch 20/50\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 1.1183 - accuracy: 0.5902\n",
      "Epoch 21/50\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.0983 - accuracy: 0.6230\n",
      "Epoch 22/50\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 1.0666 - accuracy: 0.6284\n",
      "Epoch 23/50\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 1.0482 - accuracy: 0.6284\n",
      "Epoch 24/50\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.0129 - accuracy: 0.6448\n",
      "Epoch 25/50\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.9916 - accuracy: 0.6612\n",
      "Epoch 26/50\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.9544 - accuracy: 0.6831\n",
      "Epoch 27/50\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.9303 - accuracy: 0.6940\n",
      "Epoch 28/50\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.9008 - accuracy: 0.7049\n",
      "Epoch 29/50\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.8841 - accuracy: 0.7049\n",
      "Epoch 30/50\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.8586 - accuracy: 0.7158\n",
      "Epoch 31/50\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.8421 - accuracy: 0.7213\n",
      "Epoch 32/50\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.8182 - accuracy: 0.7377\n",
      "Epoch 33/50\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.8047 - accuracy: 0.7268\n",
      "Epoch 34/50\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.7872 - accuracy: 0.7432\n",
      "Epoch 35/50\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.7705 - accuracy: 0.7596\n",
      "Epoch 36/50\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.7589 - accuracy: 0.7541\n",
      "Epoch 37/50\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.7476 - accuracy: 0.7760\n",
      "Epoch 38/50\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.7428 - accuracy: 0.7705\n",
      "Epoch 39/50\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.7492 - accuracy: 0.7923\n",
      "Epoch 40/50\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.7273 - accuracy: 0.7760\n",
      "Epoch 41/50\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.7114 - accuracy: 0.7978\n",
      "Epoch 42/50\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.7023 - accuracy: 0.8251\n",
      "Epoch 43/50\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.6860 - accuracy: 0.8142\n",
      "Epoch 44/50\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.6732 - accuracy: 0.8525\n",
      "Epoch 45/50\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.6675 - accuracy: 0.8579\n",
      "Epoch 46/50\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.6555 - accuracy: 0.8415\n",
      "Epoch 47/50\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.6451 - accuracy: 0.8798\n",
      "Epoch 48/50\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.6343 - accuracy: 0.8962\n",
      "Epoch 49/50\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.6252 - accuracy: 0.8634\n",
      "Epoch 50/50\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.6143 - accuracy: 0.9126\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a1a8d6c400>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97efc00e-2bc8-4ff8-85fd-bb42dfe17a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets test the model - I am going to write the sentences to test\n",
    "test = ['i feel good', 'i feel very bad', \n",
    "        'lets eat dinner']\n",
    "# lets convert the words into padded tokenize sequences\n",
    "test_sequ = tokenizer.texts_to_sequences(test)\n",
    "x_test = pad_sequences(test_sequ, maxlen=maxlen, padding='post', \n",
    "                      truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3db3529f-fa86-4e6d-9649-8e13c18249f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.23779343, 0.02844701, 0.44332972, 0.27672336, 0.01370649],\n",
       "       [0.10609159, 0.03752771, 0.41416153, 0.42035443, 0.02186473],\n",
       "       [0.07844795, 0.11742983, 0.05582644, 0.10224558, 0.6460502 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets make the predictions\n",
    "y_predict = model.predict(x_test)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba79c6f1-5d33-4763-957e-c0532f3fc5d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 4], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # lets get the index where max value is present\n",
    "y_pred = np.argmax(y_predict, axis=1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b0bcc76-815f-4213-87b8-dc0731c8f86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i feel good üòÉ\n",
      "i feel very bad üòû\n",
      "lets eat dinner üçΩÔ∏è\n"
     ]
    }
   ],
   "source": [
    "# we have our tokens, lets see the emojies now\n",
    "for i in range(len(test)):\n",
    "    print(test[i], label_to_emoji(y_pred[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2dda375-fde6-4200-b11e-62e2a6feed4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this was just an initial level implementation - model is making mistakes\n",
    "# we can use large dataset to improve the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd06707-87ca-4ea5-8103-c45705cef8e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
